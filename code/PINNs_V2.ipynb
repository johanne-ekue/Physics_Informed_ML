{"cells":[{"cell_type":"markdown","metadata":{"id":"1b5HNGo7LvOJ"},"source":["# A Physics-Informed Neural Networks for System Identification\n","# Case Study: The Simple Pendulum\n","\n","\n","## Author (put your details)\n"," + Johanne Naa Ayeley Ekue\n","     + Researcher \n","     + [Github Physics Project]https://github.com/johanne-ekue\n","     + Email: jnae.ekue@gmail.com\n","     \n","     \n","\n","\n","## Objectives (work on this)\n","\n","\n","+ Learn how to solve ODEs with neural networks using Physics.\n","+ See how the simulated neural network and physics based neural networks.\n","+ Introduce the problem of spectral bias in DNN's and see how we can solve it.\n","\n","## References (put the papers you read here the same way)\n","\n","+ [Artificial Neural Networks for Solving Ordinary and Partial Differential Equations](https://arxiv.org/pdf/physics/9705023.pdf)\n","+ [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n","+ [On the Spectral Bias of Neural Networks](http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf)\n","+ [Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains](https://arxiv.org/pdf/2006.10739.pdf)\n","\n","\n","<b>copy this line and edit this for bold text</b>"]},{"cell_type":"markdown","metadata":{"id":"zhDQvfygLvOU"},"source":["##Single order ODE\n","#Introduction\n","Physics informed neural network has received massive attention in the field of science and engineering. Physics Informed Neural Network(PINN) are neural networks(NN) that encodes model equations, like  Partial differential Equations(PDE), as a component of the neural network(Cuomo,s. et al(2022)).Using neural networks as surrogate models that have been trained using data gathered at a combination of input and output values is fundamentally different from PINNs. PINN has a wide range of potential applications in many fields due to its ability to incorporate physical constraints and laws into the training process of the neural network.Due to its simplicity, PINNs have helped advance several branches of computer science and engineering. PINNs are getting increased attention in the engineering and scientific literature for solving various differential equations with applications in weather modeling, healthcare, manufacturing, and other sectors.(lawal et al 2022).The intelligent and effective simulation and control of intricate real-world systems is becoming more and more crucial in the age of industry 4.0.PINN can also be used to solve problems such as strutural mechanics, fluid dynamics, heat transfer, climate modeling prediictive maintenance, financial modelling,space explorations,data assimilation, and optimization.The PINN approach essentially transforms the difficulty of directly solving the governing equations into a loss function optimization problem in order to identify ODE solutions.\n","\n","PINNs consider the underlying ODE of our dynamic system (damped pendulum), i.e., the physics of the problem, rather than attempting to infer the solution purely from the data.According to Lagaris et al the best parameter values are determined using a neural network, and that the solution of a differential equation is stated as a constant term and an adjustable term with unknown parameters.\n","According to Cuomo,s. et al, physics inforned neural networks can address problems that are described by few data, or noisy experiment observations.\n","This introduces the era of digital twin:Performant and expressive computer simulations models able to seemlessly incorporate physical measurements.(Rosen et al.,2015).According to Mauel A. et al.data based modelling and physical models have limitations and to reduce model bias and bridge the gap  between both model types which includes physics informed neural network.\n","\n","This paper focuses on using Ordinary Differential Equation(ODE) for our physics informed neural network to optimize our dynamical system (damped simple pendulum) to improve it numerically. The underlying physical laws governing the damped pendulum are incorporated into the architecture of the neural network by training the neural network to minimize the loss functions\n","\n","#Methodology\n","The simulation of the damped pendulum of this paper was done by the use of ordinary differential equations.An ordinary differential equation (ODE) is a mathematical equation that relates an independent variable to a function of one variable and its derivatives. It is used to simulate a variety of natural phenomena, including as population expansion, electrical circuits, and chemical reactions. An ODE's solution reveals information on how, given the initial conditions described, the dependent variable changes over time.The order of the highest  differential cooefficient which is involved.When an equation is a polynomial in all the differential coefficient involved, the power to which the highest differential coefficient  is raised is known as the degree of the equation.Differential equations have been solved using a variety of techniques up to now. Some of them generate an array-based solution that contains the value of the solution at a chosen set of points.\n","Others transform the original and represent the result in analytical form using basis-functions.(legaris et al 1997).\n","\n","ODE of the second order:\n","\n","$$\n","\\frac{d^2y}{dx^2} + y ={x^3}\n","$$\n","ODE of the first order:\n","$$\n","{(x +y)^2}\\frac{dy}{dx}=1\n","$$\n","##Simple Pendulum\n","The dynamical system used as a case study for this paper is simple damped pendulum.The pendulum model is a classic model in physics and has important theoretical and practical significane(Jin Wang et al. 2022). A damped simple pendulum is an ideal case because of its deterministics behaviour due to its motion which is determined by its intial conditions and the laws of physics governing its  behavious,its complex behaviour which includes its oscillations, decay and eventually stopping which can be described mathematically using differential equations.The term \"damping\" refers to any impact that has the tendency to dissipate the system's energy through any kind of resistance. The ratio between damping force and relative velocity is represented by the damping coefficient.\n","A simple pendulum is an idealization of a real pendulum. It consists of an infinitely light rigid rod attached to a frictionless pivot point, and a point mass attached to the free end of rigid rod. \n","##Ordinary differential equation\n","The Ode of the damped pendulum considered in the model is\n","\n","$$\n","\\dot{\\omega} = (\\frac{-b}{m})*ω + \\frac{g}{l}sin\\theta\n","$$\n","where:\n","$$\n","\\dot{\\omega} =\\frac{d^2\\theta}{dt^2}\n","$$\n","\n","$$\n","{\\omega} =\\frac{d\\theta}{dt}\n","$$\n","The b in the equation is the damping coeffiecient, m is the mass of the mass attached to the rod and the g is the acceleration due gravity,and l is the length of the inelastic string.\n","\n","##Predicting Motion of The Pendulum Using Neural Network\n","Linear regression model is a model that can capture linear relationships between inputs and outputs.\n","Two codes was writen for the motion of the pendulum.Linear regression and neural network were used for the first instance with respect to ODE of the pendulum.The lineare regressiom model is a statiscal approach to model the relationship of our Θ and time for 2o seconds. We obtained a mean squared error which will be further dicussed in the results and analysis sector.The advantage of this method is that it might not work well for non linear relationships has its has a low capacity for capturing interactions between features.\n","\n","##The Neural Network Model\n","A neural network model provides a more complex model that can captire non-linear relationships between inputs and outputs.It has high capacity for capturing complex interactions between features and can handle non-linear relationships.\n","The code written trained a linear regressision model \n","\n","The  code used to simulate the motion of the pendulum was used as a means of  building a neural network model to predict the motion of a damped pendulum. It starts by defining a sequential neural network model with several dense layers. The model takes the input with 3 features, and the output is 2-dimensional (the angle and angular velocity of the pendulum). The model is then compiled using mean squared error as the loss function and Adam optimizer with a specified learning rate and \"amsgrad\" setting. The model is trained using the X_train data for 1000 epochs with a batch size of 2 and a validation split of 1%. The model performance is evaluated using the mean squared error between the predicted and actual test values. The results of the model prediction are plotted and compared to the true values in several different plots to visually assess the performance of the model.\n","This method requires more data and computing resources to train and can be more difficult to optimize compared to a simple linear model.\n","Additionally, it might be prone to overfitting, which means that is memorizes the training data instead of leanring general patterns\n","\n","\n","\n","#Physics Informed Neural Network\n","To illustrate the method used in this paper, consider the first ODE of the damped pendulum.\n","Consider the ode:\n","$$\n","\\frac{d(\\theta, ω)}{dt} = f(\\theta,ω),t)\n","$$\n","with $x \\in R^2 $ and initial conditions (IC):\n","$$\n","x(0) = [\\theta_0,\\omega_0]^T\n","$$\n","We write the trial solution by:\n","$$\n","[\\hat{\\theta_t},\\hat{\\omega_t}]^T = [\\theta_0,\\omega_0]^T +  N(\\theta,\\omega)*dt\n","$$\n","or\n","$$\n","[\\hat{\\theta_t},\\hat{\\omega_t}]^T = ode45([\\theta_0,\\omega_0]^T, N(\\omega, \\theta), \\Delta_t)\n","$$\n","where $N(\\omega, \\theta)$ is a neural network (NN).\n","\n","The solution $[\\hat{\\theta_t},\\hat{\\omega_t}]^T$ automatically satisfies the initial conditions.\n","\n","The loss function we would like to minimize to train the NN is:\n","$$\n","L(\\theta) = \\int_0^1 \\left[\\frac{d(\\theta, ω)}{dt} - f(\\hat{\\theta},\\hat{\\omega},t)\\right]^2dt\n","$$\n","\n","$NB$ The trial solution using the neural network may not give the numeric accuracy of the system. "]},{"cell_type":"markdown","metadata":{"id":"4k9_MtIFHfwh"},"source":["#RK Integration Methods - Click to read more\n","https://perso.crans.org/besson/publis/notebooks/Runge-Kutta_methods_for_ODE_integration_in_Python.html\n","\n","simulate a discretized damped pendulum with states theta and omega in Python using 4th order Runge-Kutta method.\n","\n","The equation of motion for a damped pendulum is given by:\n","\n","d²θ/dt² + b/m * dθ/dt + g/L * sin(θ) = 0\n","\n","where θ is the angular displacement, t is time, b is the damping coefficient, m is the mass, g is the acceleration due to gravity and L is the length of the pendulum.\n","\n","To discretize the equation, we use the following approximations:\n","\n","dθ/dt ≈ (θ(t+Δt) - θ(t))/Δt\n","\n","d²θ/dt² ≈ (θ(t+Δt) - 2θ(t) + θ(t-Δt))/Δt²\n","\n","Substituting these approximations into the equation of motion, we get:\n","\n","θ(t+Δt) = 2θ(t) - θ(t-Δt) - (g/L)sin(θ(t))Δt² - (b/m)(θ(t)-θ(t-Δt))*Δt\n","\n","To solve this equation using the 4th order Runge-Kutta method, we need to compute the values of θ and ω at each time step. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31885,"status":"ok","timestamp":1677881715848,"user":{"displayName":"Desmond Hammond","userId":"17685451081689258891"},"user_tz":-60},"id":"hQDhh7FvMX4n","outputId":"1938088f-39dd-4b72-dded-dffa343173b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["We're running Colab\n","Colab: mounting Google drive on  /content/drive\n","Mounted at /content/drive\n","\n","Colab: making sure  /content/drive/My Drive/Colab Notebooks/Phyics_Informed_ML  exists.\n","\n","Colab: Changing directory to  /content/drive/My Drive/Colab Notebooks/Phyics_Informed_ML\n","/content/drive/My Drive/Colab Notebooks/Phyics_Informed_ML\n","/content/drive/My Drive/Colab Notebooks/Phyics_Informed_ML\n","\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","0 upgraded, 0 newly installed, 0 to remove and 22 not upgraded.\n"]}],"source":["try:\n","  from google.colab import drive\n","  IN_COLAB=True\n","except:\n","  IN_COLAB=False\n","\n","if IN_COLAB:\n","  print(\"We're running Colab\")\n","\n","if IN_COLAB:\n","  # Mount the Google Drive at mount\n","  mount='/content/drive'\n","  print(\"Colab: mounting Google drive on \", mount)\n","\n","  drive.mount(mount)\n","\n","  # Switch to the directory on the Google Drive that you want to use\n","  import os\n","  drive_root = mount + \"/My Drive/Colab Notebooks/Phyics_Informed_ML\"\n","  \n","  # Create drive_root if it doesn't exist\n","  create_drive_root = True\n","  if create_drive_root:\n","    print(\"\\nColab: making sure \", drive_root, \" exists.\")\n","    os.makedirs(drive_root, exist_ok=True)\n","  \n","  # Change to the directory\n","  print(\"\\nColab: Changing directory to \", drive_root)\n","  %cd $drive_root\n","  !pwd\n","  !pip install -r requirements.txt\n","  !sudo apt-get autoremove\n","\n","\n","  from IPython.display import JSON\n","  from google.colab import output\n","  from subprocess import getoutput\n","  import os\n","  \n","  #@title jQuery Terminal's [Features](https://terminal.jcubic.pl/)\n","\n","  def shell(command):\n","    if command.startswith('cd'):\n","      path = command.strip().split(maxsplit=1)[1]\n","      os.chdir(path)\n","      return JSON([''])\n","    return JSON([getoutput(command)])\n","  output.register_callback('shell', shell)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XXMG61-iAWxY"},"outputs":[],"source":["# Change as you wish\n","\n","if IN_COLAB:\n","  DATASET_FOLDER = './data/dataset/'\n","  DATA_FOLDER = './data/'\n","else:\n","  DATASET_FOLDER = './../data/dataset/'\n","  DATA_FOLDER = './../data/'\n","\n","PENDULUM_DATA = 'pendulum_data.csv'\n","\n","# myData = pd.read_csv(os.path.join(DATASET_FOLDER, PENDULUM_DATA))\n","# myData.round(decimals=6)\n","# myData = myData.astype(np.float32)\n","# myData = myData.astype(np.float16)\n","# myData.describe().transpose()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"executionInfo":{"elapsed":5698,"status":"ok","timestamp":1677881721539,"user":{"displayName":"Desmond Hammond","userId":"17685451081689258891"},"user_tz":-60},"id":"wuMOHrW0UjFS","outputId":"9c6352b3-cdc2-4c51-e129-12055e96e9cb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1080x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["import torch\n","torch.manual_seed(0)\n","import torch.nn as nn\n","import numpy as np\n","np.random.seed(0)\n","from scipy.integrate import solve_ivp\n","import matplotlib.pyplot as plt\n","from tqdm.autonotebook import tqdm\n","from IPython.display import display, clear_output\n","import matplotlib.pyplot as plt\n","from scipy.integrate import odeint\n","from time import perf_counter\n","from functools import partial\n","from PIL import Image\n","import requests\n","import math\n","import time\n","import os\n","\n","\n","## check if GPU is available and use it; otherwise use CPU\n","# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","device = torch.device(\"cpu\")\n","\n","# Define the parameters\n","m = 1.0 # mass of the pendulum\n","L = 1.0 # length of the pendulum\n","g = 9.8 # acceleration due to gravity\n","b = 0.05 # damping coefficient\n","dt = 0.001 # time step\n","tmax = 10.0 # maximum simulation time\n","\n","# Define the initial conditions\n","theta0 = 0.1 # initial angular displacement\n","omega0 = 0.0 # initial angular velocity\n","\n","# Define Pendulum Dynamic Model\n","def damp_Pen(x, b, g, l, m):\n","     theta=x[0]\n","     omega=x[1]\n","     theta_dot=omega\n","     omega_dot=-((b/m)*(omega))-((g/L)*math.sin(theta));\n","     x_dot=np.array([theta_dot,omega_dot]).reshape(-1,1)\n","     return x_dot\n","\n","def rungekutta4_step(x_Dot, x_prev, dt, args=()):\n","    k1 = x_Dot(x_prev, *args)\n","    k2 = x_Dot(x_prev + k1 * dt/2., *args)\n","    k3 = x_Dot(x_prev + k2 * dt/2., *args)\n","    k4 = x_Dot(x_prev + k3 * dt, *args)\n","    return x_prev + (dt / 6.) * (k1 + 2*k2 + 2*k3 + k4)\n","\n","\n","# Initial conditions\n","x0 = [theta0, omega0]\n","sim_time = 5           # specify simulation time for each epoch\n","sampling_period = 0.04\n","t = torch.linspace(0,sim_time,int(sim_time//sampling_period))\n","\n","# True solution\n","x_t = [np.array(x0).reshape(-1,1)]  \n","\n","# Solving the ODE\n","# x = odeint(damp_Pen,x0,t,args=(b,g,l,m))\n","for idx in range(len(t)-1):\n","  x_t.append(x_t[idx] + damp_Pen(x_t[idx], b, g, L, m)*sampling_period) # Forward Euler\n","x_t = np.array(x_t).reshape(-1,2)\n","\n","# True solution\n","X_t = [np.array(x0).reshape(-1,1)]  \n","\n","for idx in range(len(t)-1):\n","  X_t.append(rungekutta4_step(damp_Pen, X_t[idx], sampling_period, args=(b,g,L,m)))  # Forth order RK\n","X_t = np.array(X_t).reshape(-1,2)\n","\n","# Plot the results\n","plt.figure(figsize=(15,5))\n","plt.plot(t, X_t[:,0],label='Theta_RK4')\n","plt.plot(t, X_t[:,1],label='Omega_RK4')\n","plt.plot(t, x_t[:,0], '-', label='Theta_RK1')\n","plt.plot(t, x_t[:,1], '-', label='Omega_RK1')\n","plt.xlabel('Time')\n","plt.ylabel('Amplitude')\n","plt.legend(loc='best')\n","plt.grid(True)\n","plt.title(f'Simulation of Damped Pendulum {int(1/sampling_period)}Hz' )\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JLNmaalbS7Nf"},"source":["\n","Implementation of physics informed neural network with damped simple pendulum where the model does not depend on time, t as a variable but timestep and previous states such that the model takes in theta and omega and outputs omega and omega_dot. \n","Make use of of runge_kutta 4 in your descretization.\n","\n","To implement a physics informed neural network (PINN) for a damped simple pendulum that does not depend on time t but rather on the timestep and previous states, we will follow the following steps:\n","\n","Define the governing equation for the damped simple pendulum system.\n","Discretize the governing equation using the fourth-order Runge-Kutta method.\n","Define the loss function for the PINN.\n","Train the PINN using stochastic gradient descent.\n","Step 1: Define the governing equation\n","The equation of motion for a damped simple pendulum is given by:\n","\n","θ''(t) + bθ'(t) + c sin(θ(t)) = 0\n","\n","where θ is the angle of the pendulum from the vertical, \n","      b is the damping coefficient,\n","  and c is the gravitational constant. \n","  \n","To remove the time dependence from this equation, we can make use of the following relations:\n","\n","θ'(t) ≈ (θ(t) - θ(t-Δt)) / Δt\n","\n","θ''(t) ≈ (θ'(t) - θ'(t-Δt)) / Δt\n","\n","where Δt is the time step. Using these relations, we can rewrite the equation of motion as:\n","\n","(θ(t) - θ(t-Δt) - Δt θ'(t)) / (Δt^2) + b (θ(t) - θ(t-Δt)) / Δt + c sin(θ(t)) = 0\n","\n","Simplifying this expression, we get:\n","\n","θ(t) = 2θ(t-Δt) - θ(t-2Δt) - Δt^2 (bθ'(t) + c sin(θ(t)))\n","\n","This equation can be used to compute the value of θ(t) at any time t given the values of θ(t-Δt) and θ(t-2Δt).\n","\n","We can also compute the angular velocity, ω, and angular acceleration, ω_dot, from the values of θ(t) and θ'(t):\n","\n","ω(t) = (θ(t) - θ(t-Δt)) / Δt\n","\n","ω_dot(t) = (ω(t) - ω(t-Δt)) / Δt\n","\n","Step 2: Discretize the governing equation using Runge-Kutta 4\n","\n","To discretize the governing equation using the fourth-order Runge-Kutta method, we will use the following algorithm:\n","\n","Given the current state (θ(t), ω(t)), compute the k1, k2, k3, and k4 values as follows:\n","k1 = Δt ω(t)\n","k2 = Δt (ω(t) + 0.5Δt k1)\n","k3 = Δt (ω(t) + 0.5Δt k2)\n","k4 = Δt (ω(t) + Δt k3)\n","\n","Use the k values to update the state as follows:\n","θ(t+Δt) = 2θ(t) - θ(t-Δt) - Δt^2 (b ω(t) + c sin(θ(t)))\n","ω(t+Δt) = ω(t) + (1/6)(k1 + 2k2 + 2k3 + k4)\n","\n","Step 3: Define the loss function for the PINN\n","\n","The loss function for the PINN is defined as the sum of three terms:\n","\n","The mean squared error (MSE) between the predicted angular velocity, ω_pred, and the actual angular velocity, ω_actual, at the current time step:\n","\n","MSE(ω_pred, ω_actual) = (ω_pred - ω_actual)^2\n","\n","The MSE between the predicted angular acceleration, ω_dot_pred, and the actual angular acceleration, ω_dot_actual, at the current time step:\n","MSE(ω_dot_pred, ω_dot_actual) = (ω_dot_pred - ω_dot_actual)^2\n","\n","The residual of the governing equation, which should be close to zero if the neural network is accurately representing the physics of the system. To compute this residual, we can evaluate the governing equation at a random set of points within the domain of the system, and then take the MSE between the predicted residual, R_pred, and the actual residual, R_actual:\n","R_pred = 2θ(t) - θ(t-Δt) - Δt^2 (b ω(t) + c sin(θ(t)))\n","R_actual = 0\n","\n","The total loss function is the sum of these three terms:\n","\n","Loss = MSE(ω_pred, ω_actual) + MSE(ω_dot_pred, ω_dot_actual) + MSE(R_pred, R_actual)\n","\n","Step 4: Train the PINN using stochastic gradient descent\n","\n","To train the PINN, we can use stochastic gradient descent to minimize the loss function. We can initialize the neural network with random weights and biases, and then update these parameters using backpropagation. During each iteration of the training process, we randomly sample a set of time steps from the data set and use these as inputs to the neural network. We then compute the predicted values of ω and ω_dot, and use these to compute the loss function. Finally, we update the weights and biases using the gradient of the loss function with respect to the parameters.\n","\n","After training, the PINN should be able to accurately predict the motion of the damped simple pendulum system given its initial state and any external forces acting on it.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJL8lZoaXjQL"},"outputs":[],"source":["# Define the parameters of the system\n","m = 0.05   # mass of the pendulum\n","g = 9.81   # acceleration due to gravity\n","L = 1.0    # length of the pendulum\n","b = 0.02   # damping coefficient\n","\n","# Define the time interval and time step\n","t0 = 0.0\n","tmax = 8.0\n","dt = 0.005\n","\n","# Define the number of hidden layers and hidden units in the neural network\n","hidden_layers = 5\n","hidden_units = 200\n","\n","class PINN(nn.Module):\n","    def __init__(self, dropout_prob=0.0):\n","        super(PINN, self).__init__()\n","        self.fc1 = nn.Linear(2, hidden_units)\n","        self.hidden = nn.ModuleList()\n","        for i in range(hidden_layers - 1):\n","            self.hidden.append(nn.Sequential(\n","                nn.Linear(hidden_units, hidden_units),\n","                nn.Tanhshrink(),\n","                nn.Dropout(p=dropout_prob) ))\n","\n","        self.fc2 = nn.Linear(hidden_units, 2, bias=True)\n","        self.dt = torch.tensor(dt, dtype=torch.float32)\n","\n","    def forward(self, x):\n","        theta_prev, omega_prev = x[:, 0:1], x[:, 1:2]\n","        x = torch.tanh(self.fc1(x))\n","        for layer in self.hidden:\n","            x = layer(x)\n","        x = self.fc2(x)\n","        x = self.damp_Pendulum(x, theta_prev, omega_prev)\n","        return x\n","\n","    def damp_Pendulum(self, x, theta, omega):\n","        # theta += x[:,0]\n","        # omega += x[:,1]   \n","        return torch.cat(\n","                          [omega+x[:,1], \n","                          (-(b/m)*(omega+x[:,1]) -(g/L)*torch.sin(theta+x[:,0]))], \n","                         dim=1)\n","\n","\n","# Define the loss function\n","def loss_function(theta_pred, theta_actual, omega_pred, omega_actual, omega_dot_pred, omega_dot_actual, residual_pred, residual_actual):\n","    loss = nn.MSELoss()(theta_pred, theta_actual)\n","    # loss += nn.MSELoss()(omega_pred, omega_actual)\n","    loss = nn.MSELoss()(residual_pred, residual_actual)\n","    # loss += nn.MSELoss()(omega_dot_pred, omega_dot_actual)\n","    return loss\n","\n","# Define the function that computes the residual of the differential equation\n","def residual_function(theta, omega, omega_dot):\n","    return omega_dot + (b/m)*omega + (g/L)*torch.sin(theta)\n","\n","\n","# Define the function for the derivative of theta and omega\n","def damp_Pendulum(states):\n","    theta = states[:, 0].item()\n","    omega = states[:, 1].item()\n","    return torch.tensor([[omega, -(b/m)*omega -(g/L)*torch.sin(torch.tensor(theta))]], dtype=torch.float32)\n","\n","\n","def pytorch_rolling_window(x, window_size=10, step_size=1):\n","    # unfold dimension to make our rolling window\n","    return x.unfold(0,window_size,step_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n9-h_ef17CVR"},"outputs":[],"source":["# Initialize the PINN and the optimizer\n","model = PINN()\n","torch.save(model.state_dict(), 'model_weights.pth')  \n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["f6dd9669a4ea48968a9fc5bba3f867e6","52315fd26bdd487b9c32862c6aedcbfe","62a6f9366a374c5c8483933f40bc7bf7","a1227982dd9a46d1bde7e68040cb90a1","b34baf01a88141e5923630db1b7e0f6f","6328b85ce9f244f1afc26545b005e6f5","4cb6ae5c2ce94350af27504f0708b738","bbf12916900d4e928b01e3f832ac9de2","19a52192be46414d8bfd45e32b71d989","3484a29efc4e4100af871edfb725d870","c626ef44d5a14987b0a05cb39428fd98","b4a45ceb540b4fad9be7d05aab115fc6","8f9f886562b2463896306873434b8bc2","fc05af248f3a410190315da7b3c89c3b","046dbe14e7634640a5856afc579f1057","824265b31b84403fa1d9e5efc2845b11","154c5ac95d224335a80b4b920146a9dd","868a9d3ad614451ba8c297a1e88f645c","6ff923f134f148ac8b487e0f6ba23cb1","316429c2ad5e4913b8d928e0d6cb1b72","3ccb3da09bee4b4195cb04fbfedb5def","89a7cf2e45b84fad996248c30bc9c6eb","d8d3df87dd9b4b8b9caaba9ffb811d37","87b873333b7e49e3a740c4a6ad20bbd1","c09d5000e80e4923af0e30c2fc9742d5","615917daa2c5433ca67b6fadb227d5ce","4bf5d4dbf3cf4baeb0fb26f83899bac9","ae7fc880db6d4afe91d6073b1873f978","1f74114e9a68464eabef8fbbe213d69d","eff314a726f94aa5a3c1ade5e1b0cdc7","5647d0bb250d482f90020c197e64a668","d90c76faecff43f98718dc3d2ccc5e8e","9152de694c974a3ca9a4943ad7d68cc1","ac2de50d560e45dba02a9c5234214881","2aee738559844fe3a16e639a46b409d6","c4ba64860f264c7db085d74c18279a18","e9eedacf1fd24a77986f5d7c746107a4","ddc72aa36d4249b8b1bc3b7786661696","86b1e11e01e648a595489642b7265cbf","49e57733793946b4bcbe1a0c63a10744","742d7d14029e4fc69e2a08566af9cb34","80364bf0e66446d6874dc4e4901ae5f8","3992b684111847689c0ef7836e80ed2a","4773e75053dd4a5c87681aebbe30901d","995fd94e3ee54f93850f1f95670c9384","bb42917a8ee04d6f91cce5ba5db3b1d1","e6f4353a1551435b90e38440acef172b","a85af070ce39407c8cdf56cdf2e045de","28898b0649a149afb2d6ef5ce7b247e8","06939cfaf1fc4886b8f2e7ff1a2b228f","24086fcbfb984504a01fed20925dfa34","acfa892c4aba478388fe2f1cbead4364","ca9edadaecdc487d9dc6bf5f4745e000","ca06e55dca03490fb85b6dbababa6248","dc7e14ac33494051a842f2400cac2b84","d992b7eb37194f5ab98267064a14a8eb","ad3fc896be4e44558cb6afd8da893c93","4e5d073f34084184b3f513d8ffad5860","d907859afc504eeca24dae37b4473efc","bd0725bcb75540e9bdbadbccabf66ad3","ebd27a23888e49cda189653c6e23db4f","17199da3363443f2b5c954bcb23cc851","956c4125e78b4bf6a23d1f2a94745db7","ac340ec5ce0643e5b85825e1932ddbbc","13eb9bc705ab4b1db11ef5563d9fbc21","5469735a236f47b08dbdc200bff11b1d","5db5329cfdf6470aa5001a6915d5e41e","5dbe6f822e5c48eea419ad835dcb9de2","71fbb66750494cbeb54e6231655eba18","05905b08de0344b19eff931f94d4bbd8","a69508dd4516429aa34eab73c68c4faa","c89044cd917546ed8b136530511cf613","cf4fa89f68df4305a0e72901d676bb01","ff504286e0ad4da4af70c82e6edd8d55","5a3c413bb61848039e5950c83c64ed9a","4f868ca8e92e48178a0f4ee203bc9cf3","11f50c78362b4885a53d7889b6875175","0529760966984790800db92cdb8b4e07","c8dc77f9fa39470ab19b9bdcc2289355","4803d4094f664123b0afa6deee10cd7e","bc8fbfb3adaa4755a5eeebed932b2aab","1bc90544697145af9634d0f5446b38fe","3c47d4c7f76c40de87a65be237209884","f27295f8975049b98d030e963d0d6aaa","ed046ee7ce3746e8ab9fa17837fd505c","0656baf49a4547c89900e6cd72ae73ef","38471fd75a42460cb29ed6dd07b6fd20","4392703b66d64b0d89d57f0bb9e931af","f56179127f9c4baa8bcb247cbf0f92c6","a16452ac7a5345148a218eef756c8313","4d8aa4f33e134981a65ee0f00eee67a5","16a9fe09de4645d1b40feacc901faea3","82ee794409b5494e84ad33f1db42f015","44a8ed5d7b84485dafbcd6c68ae9ffe6","980e60867fe64078ae57ff0692eb35eb","3650fed6ffa24a64ae7d9eadfde22686","9a870d20fe284afab2ee2d385db8df27","65d1e0529f584e48a76c57fdd7a089e1","3044badb741e4d9dafe15ddc0130d820","7add205d98ef49509dca52b02f536462","e45d42ff8b0a4bdd8dbeaa8bb06d3724","fa61776497df43cd945b1c91d845576b","c3c0227675e1465c97c082754edb62dc","de478ba1854c4d64bd2977f58d6d2dbf","8a213e287bbe467fa2a3842478b8765c","6de62069d06f487cb3a5306b98f38bf0","f3b73a8c41684c0e82dfde81154dcbd2","25a7776fe5864122a706c4b934ef7cd3","a44a012f368742a8b4005ede7fbc5c41","e22511f44a49460880f670f950bcd45e","51c3d58078dc4118a72633615d5ba72e","7201fbce23d949f197c19a81ca1fce4b","f11725a748684a9d9ff547e2ef2fb761","c4f170c0d3014fca95dcc9f9662ad5d4","333def2aed184b299c436088281ca8cc","ae875200ef7b4c7cbc0bbae1d270d110","f55b67583360460e9c4c065e0f97dc36","6f74be5a60784a778f50fd4d27cc1353","e9d2cb7e3e764c4c9774786d561f5d74","bc2ae44e59444febb7f1f9727e5493c2","002af59acb8f40149057dee92cfbac78","7c93c30d908540428b6380dfe548132f","3b1a229b1f0f47ba911d875077a29100","bbf8da4357244ceba29a02d92bff99f3","e7f249c8a0e14a29a1175891fd10a63c","e0804d71748448069fd3f090f698d636","a5ea40597930491aa1dabb12b790a6c7","f137c7d81aa94680a36345e70bc4b641","8845edef4b61458bbadab90c8b98552c","16860f5a47df4daaae78e6fbacb91909","413d2244d6de4685b1aa37f68aefb7ff","70428e4dabb34c8d965ceeced5f9dcd9","f3bea8fa58c34accb25cee476b53e7f4","df44a2e7fd674e0486daba741a728fce","52bf2254a4eb439cb0d7324b11665e43","f4cda6662d53443cb129c78e3ca6ce95","8717e6da71004e9a9e32ce3cd556a837","742cf7dce53349288ec2da3503783ffd","544edd53bcf0487b825491044e9a7302","71bc5522b6434826a3d7aaf132631243","e04a2a0f170b49019d9f05d89eb35520","cea1c54b831b4f0a8adb6af25371f715","9c0ea85133cc49e99e7c56cae20e3a47","610319b49fe24bc1ab5e6b9e0beee98a","625daf1877f340a5be334c30e08631cd","3fe513d2172746bd8d87eb12a8f9200e","ba514096a0634363b5d9f80439df79b7","6f56ec957779454f81b74772ae191cd5","e444ff22a9334dca96e7d8158ff413f9","13a45ec56d4c4179941b6df3d753c1bf","b729d71b72ed497b92e209ce9dbc1950","f634ba2b00f340c5a169793f2f003cd7","df8dce8136d74360863cdf4668773da3","50f7c6412a69403981c079aeb7b1c8b9"],"output_embedded_package_id":"1bIzcggKIf0bmjtVDPrKoKAzaoX9-59dX"},"id":"UUENY-clntKD","executionInfo":{"status":"error","timestamp":1677882771556,"user_tz":-60,"elapsed":637184,"user":{"displayName":"Desmond Hammond","userId":"17685451081689258891"}},"outputId":"5040fcc1-1911-4e36-f892-05254f594869"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["num_epochs = 100\n","\n","for epoch in tqdm(range(1,num_epochs+1)):    \n","  k1s = []\n","  k2s = [] \n","  k3s = []\n","  k4s = []   \n","  torch.save(model.state_dict(), 'model_weights.pth')  \n","  t = np.arange(t0, tmax, dt)\n","  theta = np.zeros_like(t)\n","  omega = np.zeros_like(t)\n","  omega_dot = np.zeros_like(t)\n","  theta_Pred = np.zeros_like(t)\n","  theta_dot_Pred = np.zeros_like(t)\n","  omega_Pred = np.zeros_like(t)\n","  omega_dot_Pred = np.zeros_like(t)\n","  Loss = np.zeros_like(t)\n","  residual_actual = np.zeros_like(t)\n","  residual_Pred = np.zeros_like(t)\n","  theta[0] = 0.01*torch.pi*torch.randn(1).item()\n","  omega[0] = 0.001*torch.randn(1).item()\n","  theta_Pred[0] =theta[0]\n","  omega_Pred[0] = 0.001*torch.randn(1).item()\n","\n","  flag=True\n","  for i in tqdm(range(1,len(t))):\n","      # Previous true and estimated solutions of the ODEs\n","      outputs_prev = torch.tensor( [[theta[i-1], omega[i-1]]],  dtype=torch.float32) \n","      if epoch%5==0:\n","            outputs_Prev = outputs_prev \n","            if flag: print('Using Pendulum'); flag=False\n","      else:\n","            outputs_Prev = torch.tensor( [[theta_Pred[i-1], omega_Pred[i-1]]],  dtype=torch.float32)\n","            if flag: print('Using PINN'); flag=False\n","\n","\n","      # Use the 4th order Runge-Kutta method to compute the values of theta and omega at each time step\n","      k1 = damp_Pendulum(outputs_prev)\n","      # print(k1.shape)\n","      k2 = damp_Pendulum(outputs_prev+0.5*k1)\n","      # print(k2.shape)\n","      k3 = damp_Pendulum(outputs_prev+0.5*k2)\n","      # print(k3.shape)\n","      k4 = damp_Pendulum(outputs_prev+k3)\n","      # print(k4.shape)\n","      result = outputs_prev + (dt/6)*(k1 + 2*k2 + 2*k3 + k4)\n","      # result = outputs_prev + dt*k1\n","      theta[i] = result[:,0]\n","      omega[i] = result[:,1]\n","      omega_dot[i] = k1[:,1]\n","      residual_actual[i] = residual_function(result[:,0], result[:,1], k1[:,1])\n","\n","      # Use the 4th order Runge-Kutta method to compute the values of theta and omega at each time step\n","      model.load_state_dict(torch.load('model_weights.pth'))\n","      model.train()\n","      optimizer.zero_grad()\n","      k1_ = model(outputs_Prev)\n","      # print(k1_.shape)\n","      k2_ = model(outputs_Prev+0.5*k1_)\n","      # print(k2_.shape)\n","      k3_ = model(outputs_Prev+0.5*k2_)\n","      # print(k3_.shape)\n","      k4_ = model(outputs_Prev+k3_)\n","      # print(k4_.shape)\n","      result_ = outputs_Prev + (dt/6)*(k1_ + 2*k2_ + 2*k3_ + k4_)\n","      # result_ = outputs_Prev + dt*k1_\n","      theta_Pred[i] = result_[:,0]\n","      omega_Pred[i] = result_[:,1]\n","      omega_dot_Pred[i] = k1_[:,1]\n","      residual_Pred[i] = residual_function(result_[:,0], result_[:,1], k1_[:,1])\n","\n","      k1s.append( [k1_.detach().numpy(), k1.detach().numpy()] )\n","      k2s.append( [k2_.detach().numpy(), k2.detach().numpy()] )\n","      k3s.append( [k3_.detach().numpy(), k3.detach().numpy()] )\n","      k4s.append( [k4_.detach().numpy(), k4.detach().numpy()] )\n","\n","      loss = loss_function( torch.unsqueeze(result_[:,0],1),                      torch.unsqueeze(result[:,0],1),\n","                            torch.unsqueeze(result_[:,1],1),                      torch.unsqueeze(result[:,1],1), \n","                            torch.unsqueeze(k1_[:,1],1),                          torch.unsqueeze(k1[:,1],1), \n","                            torch.unsqueeze(torch.tensor([residual_Pred[i]]),1),  torch.unsqueeze(torch.tensor([residual_actual[i]], requires_grad=True),1))\n","      loss.backward()\n","      optimizer.step()\n","      torch.save(model.state_dict(), 'model_weights.pth')  \n","      Loss[i] = loss\n","      # print(\"omega = {:.4f}, omega_pred = {:.4f}, omega_dot = {:.4f}, omega_dot_pred = {:.4f}, loss = {:}\".format(omega[i].item(), omega_Pred[i].item(), omega_dot[i].item(), omega_dot_Pred[i].item(), loss.item()))\n","\n","  # Print the final state of the system\n","  # print(\"theta = {:.4f}, theta_pred = {:.4f}, omega = {:.4f}, omega_pred = {:.4f}, loss = {:}\".format(theta[-1].item(), theta_Pred[-1].item(), omega[-1].item(), omega_Pred[-1].item(), loss.item()))\n","  # print(\"omega = {:.4f}, omega_pred = {:.4f}, omega_dot = {:.4f}, omega_dot_pred = {:.4f}, loss = {:}\".format(Omega[-1].item(), omega_pred[-1].item(), Omega_dot[-1].item(), omega_dot_pred.item(), loss.item()))\n","\n"]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import matplotlib.pyplot as plt\n","\n","\n","# Define the parameters of the system\n","m = 0.05   # mass of the pendulum\n","g = 9.81   # acceleration due to gravity\n","l = 1.0    # length of the pendulum\n","b = 0.02   # damping coefficient\n","dt = 0.001  # sampling period\n","hidden_layers = 5 # number of hidden layers\n","hidden_units = 200 # number hidden units \n","L = 1.0 # Lipschitz constant\n","\n","def rungekutta4_step(x_Dot, x_prev, dt, args=()):\n","    k1 = x_Dot(x_prev, *args)\n","    k2 = x_Dot(x_prev + k1 * dt/2., *args)\n","    k3 = x_Dot(x_prev + k2 * dt/2., *args)\n","    k4 = x_Dot(x_prev + k3 * dt, *args)\n","    return x_prev + (dt / 6.) * (k1 + 2*k2 + 2*k3 + k4)\n","\n","def pendulum_dynamics(state):\n","    v, omega = state\n","    theta_dot = omega\n","    theta = torch.arcsin(v)\n","    omega_dot = (-b/m) * omega - (g/l) * torch.sin(theta)\n","    return torch.stack([theta_dot, omega_dot])\n","\n","class PINN(nn.Module):\n","    def __init__(self, dropout_prob=0.0):\n","        super(PINN, self).__init__()\n","        self.fc1 = nn.Linear(2, hidden_units)\n","        self.hidden = nn.ModuleList()\n","        for i in range(hidden_layers - 1):\n","            self.hidden.append(nn.Sequential(\n","                nn.Linear(hidden_units, hidden_units),\n","                nn.Tanh(),\n","                nn.Dropout(p=dropout_prob)))\n","\n","        self.fc2 = nn.Linear(hidden_units, 4, bias=True)\n","        self.dt = torch.tensor(dt, dtype=torch.float32)\n","        self.A_nn = torch.zeros([2,2], requires_grad=True)\n","\n","    def forward(self, x):\n","        state_prev = x \n","        x = torch.tanh(self.fc1(x))\n","        for layer in self.hidden:\n","            x = layer(x)\n","        x = self.fc2(x)\n","        x[0] = torch.tensor([0.], requires_grad=False)\n","        x = self.pendulum_dynamics(x,state_prev)\n","        return x\n","\n","    # def pendulum_dynamics(self, x, state):\n","    #     theta, omega = state\n","    #     delta_theta, delta_omega = x\n","    #     theta_dot = omega + delta_omega\n","    #     omega_dot = (-b/m) * (omega + delta_omega) - (g/l) * torch.sin(theta + delta_theta)\n","    #     return torch.stack([theta_dot, omega_dot])\n","\n","    def pendulum_dynamics(self, x, state_prev):\n","        v, omega = state_prev\n","        self.A_nn = torch.tensor([[0., 1.], [-g/l, -1/m]], requires_grad=True)-x.reshape(2,2)\n","        theta_dot, omega_dot = torch.matmul(self.A_nn, torch.stack([v, omega]))\n","        return torch.stack([theta_dot, omega_dot])\n","\n","# define the Lipschitz constant\n","def lipschitz_constant(net, norm='fro'):\n","    # compute the Jacobian matrix\n","    x = torch.randn(2, requires_grad=True)\n","    y = net(x)\n","    J = torch.autograd.functional.jacobian(net, x)\n","    # compute the norm of the Jacobian matrix\n","    L = torch.norm(J, p=norm)\n","    return L\n","\n","# Initialize the PINN and the optimizer\n","model = PINN()\n","torch.save(model.state_dict(), 'model_weights.pth')  \n","learning_rate = 1e-3\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, amsgrad=True)\n","\n","state0 = torch.tensor([0.1, 0.0], requires_grad=True) # initial state v and omega \n","state0_NN = torch.tensor([0.1, 0.0], requires_grad=True) # initial state v and omega \n","\n","for _ in range(10000):\n","  #### Step once ####\n","  model.load_state_dict(torch.load('model_weights.pth'))\n","  model.train()\n","  optimizer.zero_grad()\n","  # v1, omega1 = state1  = state0 + pendulum_dynamics(state0)*dt           # 1st order Runge-Kutta Integration (Forward Euler)\n","  # v1_NN, omega1_NN = state1_NN = state0 + model(state0_NN)*dt                    # 1st order Runge-Kutta Integration (Forward Euler)       \n","  v1, omega1 = state1 = rungekutta4_step(pendulum_dynamics, state0, dt)   # 4th order Runge-Kutta Integration \n","  v1_NN, omega1_NN = state1_NN = rungekutta4_step(model, state0_NN, dt)      # 4th order Runge-Kutta Integration \n","\n","  theta1 = torch.asin(v1)\n","  theta1_NN = torch.asin(v1_NN)\n","\n","  f0 = pendulum_dynamics(state0)\n","  f1 = pendulum_dynamics(state1)\n","\n","  f0_NN = model(state0_NN)\n","  f1_NN = model(state1_NN)\n","\n","  J0 = torch.autograd.functional.jacobian(pendulum_dynamics, state0)\n","  J1 = torch.autograd.functional.jacobian(pendulum_dynamics, state1)\n","  J0_NN = torch.autograd.functional.jacobian(model, state0_NN)\n","  J0_NN = model.A_nn\n","  J1_NN = torch.autograd.functional.jacobian(model, state1_NN)\n","  # J1_NN = model.A_nn\n","\n","  Jf0 = torch.matmul(J0, state0)\n","  Jf0_NN = torch.matmul(J0_NN, state0_NN)\n","\n","  \n","\n","\n","  # Define the loss function with Lipschitz regularization\n","  norm='fro'\n","  # compute the mean squared error loss\n","  loss = nn.L1Loss()(theta1, theta1_NN)\n","  # compute the Jacobian matrix loss\n","  loss += 0.001*torch.norm(J0_NN, p='fro')\n","   # # compute the Lipschitz constant\n","  # L = lipschitz_constant(model, norm=norm)\n","  # # compute the gradient of the loss function\n","  # grad = torch.autograd.grad(loss, model.parameters(), retain_graph=True)\n","  # grad = torch.cat([g.flatten() for g in grad])\n","  # # compute the penalty term\n","  # lmbda=0.\n","  # grad_penalty = 0.5 * lmbda * L * torch.norm(grad)**2\n","  # # add the penalty term to the loss\n","  # loss += grad_penalty\n","\n","  loss.backward(retain_graph=True)\n","  optimizer.step()\n","  torch.save(model.state_dict(), 'model_weights.pth')  \n","  print(f'\\nLoss: {loss}')\n","\n","  print('\\nInitial state:', state0)\n","  print('Initial state NN:', state0_NN)\n","  print('New state:', state1)\n","  print('New state NN:', state1_NN)\n","\n","  print('\\nDynamics at initial state:', f0, Jf0)\n","  print('Dynamics at initial state NN:', f0_NN,Jf0_NN)\n","\n","  # print('Dynamics at new state:', f1)\n","  # print('Dynamics at new state NN:', f1_NN)\n","\n","  print('\\nJacobian matrix initial:', J0)\n","  print('Jacobian matrix initial NN:', J0_NN)\n","  # print('Jacobian matrix new:', J1)\n","  # print('Jacobian matrix new NN:', J1_NN)\n","  \n","  state0 = torch.tensor(state1, requires_grad=True) # initial state v and omega \n","  state0_NN = torch.tensor(state1_NN, requires_grad=True) # initial state v and omega \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"L7SKH0qjxQer","executionInfo":{"status":"error","timestamp":1678192148673,"user_tz":-60,"elapsed":11046,"user":{"displayName":"Desmond Hammond","userId":"17685451081689258891"}},"outputId":"6a28890f-54fd-40e6-8516-f4a23ccf56d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loss: 0.022385086864233017\n","\n","Initial state: tensor([0.1000, 0.0000], requires_grad=True)\n","Initial state NN: tensor([0.1000, 0.0000], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0010], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0010], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([ 0.0000, -0.9810], grad_fn=<StackBackward0>) tensor([ 0.0000, -0.9810], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0000, -0.9846], grad_fn=<StackBackward0>) tensor([ 0.0000, -0.9846], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.9004],\n","        [ -9.8463, -20.0831]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.022043175995349884\n","\n","Initial state: tensor([ 0.1000, -0.0010], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0010], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0020], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0019], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0010, -0.9806], grad_fn=<StackBackward0>) tensor([-0.0010, -0.9806], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-8.8949e-04, -9.4997e-01], grad_fn=<StackBackward0>) tensor([-8.8949e-04, -9.4997e-01], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.9125],\n","        [ -9.6925, -19.7767]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.021601609885692596\n","\n","Initial state: tensor([ 0.1000, -0.0020], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0019], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0029], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0028], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0020, -0.9802], grad_fn=<StackBackward0>) tensor([-0.0020, -0.9802], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0017, -0.9128], grad_fn=<StackBackward0>) tensor([-0.0017, -0.9128], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.8989],\n","        [ -9.4989, -19.3796]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.020944932475686073\n","\n","Initial state: tensor([ 0.1000, -0.0029], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0028], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0039], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0037], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0029, -0.9798], grad_fn=<StackBackward0>) tensor([-0.0029, -0.9798], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0024, -0.8697], grad_fn=<StackBackward0>) tensor([-0.0024, -0.8697], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.8632],\n","        [ -9.2264, -18.7822]], grad_fn=<SubBackward0>)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-52-e2a8f54867ae>:157: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  state0 = torch.tensor(state1, requires_grad=True) # initial state v and omega\n","<ipython-input-52-e2a8f54867ae>:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  state0_NN = torch.tensor(state1_NN, requires_grad=True) # initial state v and omega\n"]},{"output_type":"stream","name":"stdout","text":["\n","Loss: 0.02003873698413372\n","\n","Initial state: tensor([ 0.1000, -0.0039], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0037], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0049], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0045], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0039, -0.9794], grad_fn=<StackBackward0>) tensor([-0.0039, -0.9794], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0029, -0.8205], grad_fn=<StackBackward0>) tensor([-0.0029, -0.8205], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.7935],\n","        [ -8.8665, -17.9503]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.018940191715955734\n","\n","Initial state: tensor([ 0.1000, -0.0049], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0045], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0059], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0053], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0049, -0.9789], grad_fn=<StackBackward0>) tensor([-0.0049, -0.9789], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0031, -0.7686], grad_fn=<StackBackward0>) tensor([-0.0031, -0.7686], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.6820],\n","        [ -8.4481, -16.9331]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01778888702392578\n","\n","Initial state: tensor([ 0.1000, -0.0059], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0053], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0069], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0060], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0059, -0.9785], grad_fn=<StackBackward0>) tensor([-0.0059, -0.9785], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0028, -0.7189], grad_fn=<StackBackward0>) tensor([-0.0028, -0.7189], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.5285],\n","        [ -8.0234, -15.8591]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01673402078449726\n","\n","Initial state: tensor([ 0.1000, -0.0069], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0060], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0078], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0066], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0069, -0.9780], grad_fn=<StackBackward0>) tensor([-0.0069, -0.9780], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0021, -0.6745], grad_fn=<StackBackward0>) tensor([-0.0021, -0.6745], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.3437],\n","        [ -7.6344, -14.8724]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.015857068821787834\n","\n","Initial state: tensor([ 0.1000, -0.0078], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0066], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0088], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0073], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0078, -0.9776], grad_fn=<StackBackward0>) tensor([-0.0078, -0.9776], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0010, -0.6361], grad_fn=<StackBackward0>) tensor([-0.0010, -0.6361], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.1464],\n","        [ -7.2952, -14.0556]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01516206469386816\n","\n","Initial state: tensor([ 0.1000, -0.0088], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0073], requires_grad=True)\n","New state: tensor([ 0.1000, -0.0098], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0079], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0088, -0.9771], grad_fn=<StackBackward0>) tensor([-0.0088, -0.9771], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 2.4770e-04, -6.0230e-01], grad_fn=<StackBackward0>) tensor([ 2.4770e-04, -6.0230e-01], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,  -0.0341],\n","        [ -6.9998, -13.4158]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.014610744081437588\n","\n","Initial state: tensor([ 0.1000, -0.0098], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0079], requires_grad=True)\n","New state: tensor([ 0.0999, -0.0108], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0084], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0098, -0.9766], grad_fn=<StackBackward0>) tensor([-0.0098, -0.9766], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0011, -0.5718], grad_fn=<StackBackward0>) tensor([ 0.0011, -0.5718], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,  -0.1458],\n","        [ -6.7353, -12.9181]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.014157464727759361\n","\n","Initial state: tensor([ 0.0999, -0.0108], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0084], requires_grad=True)\n","New state: tensor([ 0.0999, -0.0117], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0090], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0108, -0.9761], grad_fn=<StackBackward0>) tensor([-0.0108, -0.9761], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0014, -0.5432], grad_fn=<StackBackward0>) tensor([ 0.0014, -0.5432], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,  -0.1701],\n","        [ -6.4891, -12.5205]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.013766980729997158\n","\n","Initial state: tensor([ 0.0999, -0.0117], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0090], requires_grad=True)\n","New state: tensor([ 0.0999, -0.0127], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0095], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0117, -0.9756], grad_fn=<StackBackward0>) tensor([-0.0117, -0.9756], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0012, -0.5158], grad_fn=<StackBackward0>) tensor([ 0.0012, -0.5158], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,  -0.1293],\n","        [ -6.2529, -12.1882]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.013416117057204247\n","\n","Initial state: tensor([ 0.0999, -0.0127], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0095], requires_grad=True)\n","New state: tensor([ 0.0999, -0.0137], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0100], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0127, -0.9751], grad_fn=<StackBackward0>) tensor([-0.0127, -0.9751], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 4.5272e-04, -4.8925e-01], grad_fn=<StackBackward0>) tensor([ 4.5272e-04, -4.8925e-01], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,  -0.0477],\n","        [ -6.0224, -11.8971]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01309045311063528\n","\n","Initial state: tensor([ 0.0999, -0.0137], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0100], requires_grad=True)\n","New state: tensor([ 0.0999, -0.0147], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0104], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0137, -0.9746], grad_fn=<StackBackward0>) tensor([-0.0137, -0.9746], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0006, -0.4634], grad_fn=<StackBackward0>) tensor([-0.0006, -0.4634], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.0553],\n","        [ -5.7954, -11.6313]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.012781066820025444\n","\n","Initial state: tensor([ 0.0999, -0.0147], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0104], requires_grad=True)\n","New state: tensor([ 0.0999, -0.0156], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0109], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0147, -0.9741], grad_fn=<StackBackward0>) tensor([-0.0147, -0.9741], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0017, -0.4382], grad_fn=<StackBackward0>) tensor([-0.0017, -0.4382], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.1619],\n","        [ -5.5708, -11.3811]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.012482213787734509\n","\n","Initial state: tensor([ 0.0999, -0.0156], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0109], requires_grad=True)\n","New state: tensor([ 0.0999, -0.0166], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0113], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0156, -0.9735], grad_fn=<StackBackward0>) tensor([-0.0156, -0.9735], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0028, -0.4136], grad_fn=<StackBackward0>) tensor([-0.0028, -0.4136], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.2532],\n","        [ -5.3478, -11.1405]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.012190002016723156\n","\n","Initial state: tensor([ 0.0999, -0.0166], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0113], requires_grad=True)\n","New state: tensor([ 0.0998, -0.0176], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0117], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0166, -0.9730], grad_fn=<StackBackward0>) tensor([-0.0166, -0.9730], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0035, -0.3894], grad_fn=<StackBackward0>) tensor([-0.0035, -0.3894], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.3115],\n","        [ -5.1259, -10.9057]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.011902051977813244\n","\n","Initial state: tensor([ 0.0998, -0.0176], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0117], requires_grad=True)\n","New state: tensor([ 0.0998, -0.0186], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0120], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0176, -0.9724], grad_fn=<StackBackward0>) tensor([-0.0176, -0.9724], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0038, -0.3658], grad_fn=<StackBackward0>) tensor([-0.0038, -0.3658], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.3292],\n","        [ -4.9047, -10.6747]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.011617407202720642\n","\n","Initial state: tensor([ 0.0998, -0.0186], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0120], requires_grad=True)\n","New state: tensor([ 0.0998, -0.0195], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0124], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0186, -0.9718], grad_fn=<StackBackward0>) tensor([-0.0186, -0.9718], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0037, -0.3426], grad_fn=<StackBackward0>) tensor([-0.0037, -0.3426], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.3099],\n","        [ -4.6840, -10.4459]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01133598480373621\n","\n","Initial state: tensor([ 0.0998, -0.0195], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0124], requires_grad=True)\n","New state: tensor([ 0.0998, -0.0205], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0127], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0195, -0.9713], grad_fn=<StackBackward0>) tensor([-0.0195, -0.9713], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0033, -0.3198], grad_fn=<StackBackward0>) tensor([-0.0033, -0.3198], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[  0.0000,   0.2627],\n","        [ -4.4638, -10.2184]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.011058150790631771\n","\n","Initial state: tensor([ 0.0998, -0.0205], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0127], requires_grad=True)\n","New state: tensor([ 0.0998, -0.0215], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0130], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0205, -0.9707], grad_fn=<StackBackward0>) tensor([-0.0205, -0.9707], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0025, -0.2975], grad_fn=<StackBackward0>) tensor([-0.0025, -0.2975], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1985],\n","        [-4.2443, -9.9917]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.010784230194985867\n","\n","Initial state: tensor([ 0.0998, -0.0215], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0130], requires_grad=True)\n","New state: tensor([ 0.0997, -0.0224], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0133], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0215, -0.9701], grad_fn=<StackBackward0>) tensor([-0.0215, -0.9701], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0017, -0.2756], grad_fn=<StackBackward0>) tensor([-0.0017, -0.2756], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1291],\n","        [-4.0256, -9.7653]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.010514314286410809\n","\n","Initial state: tensor([ 0.0997, -0.0224], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0133], requires_grad=True)\n","New state: tensor([ 0.0997, -0.0234], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0224, -0.9695], grad_fn=<StackBackward0>) tensor([-0.0224, -0.9695], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0009, -0.2541], grad_fn=<StackBackward0>) tensor([-0.0009, -0.2541], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0675],\n","        [-3.8078, -9.5391]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.010248102247714996\n","\n","Initial state: tensor([ 0.0997, -0.0234], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0997, -0.0244], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0137], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0234, -0.9689], grad_fn=<StackBackward0>) tensor([-0.0234, -0.9689], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.2331], grad_fn=<StackBackward0>) tensor([-0.0003, -0.2331], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0256],\n","        [-3.5913, -9.3129]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009985125623643398\n","\n","Initial state: tensor([ 0.0997, -0.0244], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0137], requires_grad=True)\n","New state: tensor([ 0.0997, -0.0253], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0140], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0244, -0.9683], grad_fn=<StackBackward0>) tensor([-0.0244, -0.9683], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-1.4331e-04, -2.1255e-01], grad_fn=<StackBackward0>) tensor([-1.4331e-04, -2.1255e-01], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0104],\n","        [-3.3761, -9.0866]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009725002571940422\n","\n","Initial state: tensor([ 0.0997, -0.0253], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0140], requires_grad=True)\n","New state: tensor([ 0.0996, -0.0263], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0142], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0253, -0.9676], grad_fn=<StackBackward0>) tensor([-0.0253, -0.9676], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.1924], grad_fn=<StackBackward0>) tensor([-0.0003, -0.1924], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0221],\n","        [-3.1626, -8.8602]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009467651136219501\n","\n","Initial state: tensor([ 0.0996, -0.0263], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0142], requires_grad=True)\n","New state: tensor([ 0.0996, -0.0273], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0143], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0263, -0.9670], grad_fn=<StackBackward0>) tensor([-0.0263, -0.9670], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0008, -0.1728], grad_fn=<StackBackward0>) tensor([-0.0008, -0.1728], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0554],\n","        [-2.9510, -8.6337]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009213179349899292\n","\n","Initial state: tensor([ 0.0996, -0.0273], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0143], requires_grad=True)\n","New state: tensor([ 0.0996, -0.0282], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0145], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0273, -0.9663], grad_fn=<StackBackward0>) tensor([-0.0273, -0.9663], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0015, -0.1536], grad_fn=<StackBackward0>) tensor([-0.0015, -0.1536], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1014],\n","        [-2.7416, -8.4071]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008961754851043224\n","\n","Initial state: tensor([ 0.0996, -0.0282], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0145], requires_grad=True)\n","New state: tensor([ 0.0996, -0.0292], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0146], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0282, -0.9657], grad_fn=<StackBackward0>) tensor([-0.0282, -0.9657], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0022, -0.1349], grad_fn=<StackBackward0>) tensor([-0.0022, -0.1349], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1492],\n","        [-2.5346, -8.1803]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008713457733392715\n","\n","Initial state: tensor([ 0.0996, -0.0292], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0146], requires_grad=True)\n","New state: tensor([ 0.0995, -0.0302], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.1000, -0.0147], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0292, -0.9650], grad_fn=<StackBackward0>) tensor([-0.0292, -0.9650], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0027, -0.1167], grad_fn=<StackBackward0>) tensor([-0.0027, -0.1167], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1870],\n","        [-2.3305, -7.9534]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008468362502753735\n","\n","Initial state: tensor([ 0.0995, -0.0302], requires_grad=True)\n","Initial state NN: tensor([ 0.1000, -0.0147], requires_grad=True)\n","New state: tensor([ 0.0995, -0.0311], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0148], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0302, -0.9643], grad_fn=<StackBackward0>) tensor([-0.0302, -0.9643], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0030, -0.0991], grad_fn=<StackBackward0>) tensor([-0.0030, -0.0991], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.2056],\n","        [-2.1294, -7.7263]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008226705715060234\n","\n","Initial state: tensor([ 0.0995, -0.0311], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0148], requires_grad=True)\n","New state: tensor([ 0.0995, -0.0321], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0149], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0311, -0.9636], grad_fn=<StackBackward0>) tensor([-0.0311, -0.9636], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0030, -0.0819], grad_fn=<StackBackward0>) tensor([-0.0030, -0.0819], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.2007],\n","        [-1.9319, -7.4992]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007989004254341125\n","\n","Initial state: tensor([ 0.0995, -0.0321], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0149], requires_grad=True)\n","New state: tensor([ 0.0994, -0.0331], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0150], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0321, -0.9629], grad_fn=<StackBackward0>) tensor([-0.0321, -0.9629], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0026, -0.0653], grad_fn=<StackBackward0>) tensor([-0.0026, -0.0653], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1747],\n","        [-1.7381, -7.2719]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007755930535495281\n","\n","Initial state: tensor([ 0.0994, -0.0331], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0150], requires_grad=True)\n","New state: tensor([ 0.0994, -0.0340], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0150], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0331, -0.9622], grad_fn=<StackBackward0>) tensor([-0.0331, -0.9622], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0020, -0.0493], grad_fn=<StackBackward0>) tensor([-0.0020, -0.0493], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1347],\n","        [-1.5487, -7.0445]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00752799678593874\n","\n","Initial state: tensor([ 0.0994, -0.0340], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0150], requires_grad=True)\n","New state: tensor([ 0.0994, -0.0350], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0151], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0340, -0.9615], grad_fn=<StackBackward0>) tensor([-0.0340, -0.9615], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0014, -0.0339], grad_fn=<StackBackward0>) tensor([-0.0014, -0.0339], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0911],\n","        [-1.3638, -6.8171]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007305371109396219\n","\n","Initial state: tensor([ 0.0994, -0.0350], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0151], requires_grad=True)\n","New state: tensor([ 0.0993, -0.0359], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0151], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0350, -0.9608], grad_fn=<StackBackward0>) tensor([-0.0350, -0.9608], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0008, -0.0191], grad_fn=<StackBackward0>) tensor([-0.0008, -0.0191], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0555],\n","        [-1.1842, -6.5896]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007087850011885166\n","\n","Initial state: tensor([ 0.0993, -0.0359], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0151], requires_grad=True)\n","New state: tensor([ 0.0993, -0.0369], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0151], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0359, -0.9601], grad_fn=<StackBackward0>) tensor([-0.0359, -0.9601], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0006, -0.0050], grad_fn=<StackBackward0>) tensor([-0.0006, -0.0050], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0373],\n","        [-1.0101, -6.3620]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.006875044200569391\n","\n","Initial state: tensor([ 0.0993, -0.0369], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0151], requires_grad=True)\n","New state: tensor([ 0.0993, -0.0379], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0151], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0369, -0.9593], grad_fn=<StackBackward0>) tensor([-0.0369, -0.9593], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0006,  0.0083], grad_fn=<StackBackward0>) tensor([-0.0006,  0.0083], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0398],\n","        [-0.8422, -6.1343]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.006666778586804867\n","\n","Initial state: tensor([ 0.0993, -0.0379], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0151], requires_grad=True)\n","New state: tensor([ 0.0992, -0.0388], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0151], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0379, -0.9586], grad_fn=<StackBackward0>) tensor([-0.0379, -0.9586], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0009,  0.0210], grad_fn=<StackBackward0>) tensor([-0.0009,  0.0210], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0599],\n","        [-0.6811, -5.9067]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0064629255793988705\n","\n","Initial state: tensor([ 0.0992, -0.0388], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0151], requires_grad=True)\n","New state: tensor([ 0.0992, -0.0398], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0150], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0388, -0.9578], grad_fn=<StackBackward0>) tensor([-0.0388, -0.9578], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0013,  0.0328], grad_fn=<StackBackward0>) tensor([-0.0013,  0.0328], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0885],\n","        [-0.5272, -5.6790]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00626341113820672\n","\n","Initial state: tensor([ 0.0992, -0.0398], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0150], requires_grad=True)\n","New state: tensor([ 0.0991, -0.0407], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0150], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0398, -0.9571], grad_fn=<StackBackward0>) tensor([-0.0398, -0.9571], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0017,  0.0438], grad_fn=<StackBackward0>) tensor([-0.0017,  0.0438], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1136],\n","        [-0.3814, -5.4513]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.006068010348826647\n","\n","Initial state: tensor([ 0.0991, -0.0407], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0150], requires_grad=True)\n","New state: tensor([ 0.0991, -0.0417], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0149], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0407, -0.9563], grad_fn=<StackBackward0>) tensor([-0.0407, -0.9563], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0019,  0.0538], grad_fn=<StackBackward0>) tensor([-0.0019,  0.0538], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1245],\n","        [-0.2441, -5.2237]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005876622162759304\n","\n","Initial state: tensor([ 0.0991, -0.0417], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0149], requires_grad=True)\n","New state: tensor([ 0.0991, -0.0427], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0149], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0417, -0.9555], grad_fn=<StackBackward0>) tensor([-0.0417, -0.9555], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0017,  0.0629], grad_fn=<StackBackward0>) tensor([-0.0017,  0.0629], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.1160],\n","        [-0.1163, -4.9962]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005689346697181463\n","\n","Initial state: tensor([ 0.0991, -0.0427], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0149], requires_grad=True)\n","New state: tensor([ 0.0990, -0.0436], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0148], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0427, -0.9547], grad_fn=<StackBackward0>) tensor([-0.0427, -0.9547], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0014,  0.0710], grad_fn=<StackBackward0>) tensor([-0.0014,  0.0710], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000e+00,  9.1045e-02],\n","        [ 1.5745e-03, -4.7687e+00]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005506295710802078\n","\n","Initial state: tensor([ 0.0990, -0.0436], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0148], requires_grad=True)\n","New state: tensor([ 0.0990, -0.0446], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0147], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0436, -0.9539], grad_fn=<StackBackward0>) tensor([-0.0436, -0.9539], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0009,  0.0780], grad_fn=<StackBackward0>) tensor([-0.0009,  0.0780], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0596],\n","        [ 0.1087, -4.5414]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00532721821218729\n","\n","Initial state: tensor([ 0.0990, -0.0446], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0147], requires_grad=True)\n","New state: tensor([ 0.0989, -0.0455], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0146], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0446, -0.9531], grad_fn=<StackBackward0>) tensor([-0.0446, -0.9531], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0005,  0.0839], grad_fn=<StackBackward0>) tensor([-0.0005,  0.0839], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0346],\n","        [ 0.2043, -4.3143]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005151400808244944\n","\n","Initial state: tensor([ 0.0989, -0.0455], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0146], requires_grad=True)\n","New state: tensor([ 0.0989, -0.0465], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0145], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0455, -0.9523], grad_fn=<StackBackward0>) tensor([-0.0455, -0.9523], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0886], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0886], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0265],\n","        [ 0.2880, -4.0874]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004978049546480179\n","\n","Initial state: tensor([ 0.0989, -0.0465], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0145], requires_grad=True)\n","New state: tensor([ 0.0988, -0.0474], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0144], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0465, -0.9514], grad_fn=<StackBackward0>) tensor([-0.0465, -0.9514], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0005,  0.0920], grad_fn=<StackBackward0>) tensor([-0.0005,  0.0920], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0367],\n","        [ 0.3591, -3.8608]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004806530196219683\n","\n","Initial state: tensor([ 0.0988, -0.0474], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0144], requires_grad=True)\n","New state: tensor([ 0.0988, -0.0484], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0144], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0474, -0.9506], grad_fn=<StackBackward0>) tensor([-0.0474, -0.9506], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0008,  0.0942], grad_fn=<StackBackward0>) tensor([-0.0008,  0.0942], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0569],\n","        [ 0.4171, -3.6345]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004636248107999563\n","\n","Initial state: tensor([ 0.0988, -0.0484], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0144], requires_grad=True)\n","New state: tensor([ 0.0987, -0.0493], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0143], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0484, -0.9497], grad_fn=<StackBackward0>) tensor([-0.0484, -0.9497], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0010,  0.0951], grad_fn=<StackBackward0>) tensor([-0.0010,  0.0951], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0730],\n","        [ 0.4617, -3.4086]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004466529935598373\n","\n","Initial state: tensor([ 0.0987, -0.0493], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0143], requires_grad=True)\n","New state: tensor([ 0.0987, -0.0503], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0142], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0493, -0.9489], grad_fn=<StackBackward0>) tensor([-0.0493, -0.9489], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0010,  0.0946], grad_fn=<StackBackward0>) tensor([-0.0010,  0.0946], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0725],\n","        [ 0.4924, -3.1831]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004297029692679644\n","\n","Initial state: tensor([ 0.0987, -0.0503], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0142], requires_grad=True)\n","New state: tensor([ 0.0986, -0.0512], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0141], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0503, -0.9480], grad_fn=<StackBackward0>) tensor([-0.0503, -0.9480], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0008,  0.0928], grad_fn=<StackBackward0>) tensor([-0.0008,  0.0928], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0541],\n","        [ 0.5092, -2.9581]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004127730615437031\n","\n","Initial state: tensor([ 0.0986, -0.0512], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0141], requires_grad=True)\n","New state: tensor([ 0.0986, -0.0522], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0140], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0512, -0.9471], grad_fn=<StackBackward0>) tensor([-0.0512, -0.9471], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0896], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0896], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0289],\n","        [ 0.5121, -2.7336]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003958305809646845\n","\n","Initial state: tensor([ 0.0986, -0.0522], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0140], requires_grad=True)\n","New state: tensor([ 0.0985, -0.0531], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0139], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0522, -0.9462], grad_fn=<StackBackward0>) tensor([-0.0522, -0.9462], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0852], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0852], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0143],\n","        [ 0.5012, -2.5095]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0037881112657487392\n","\n","Initial state: tensor([ 0.0985, -0.0531], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0139], requires_grad=True)\n","New state: tensor([ 0.0985, -0.0540], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0138], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0531, -0.9453], grad_fn=<StackBackward0>) tensor([-0.0531, -0.9453], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003,  0.0794], grad_fn=<StackBackward0>) tensor([-0.0003,  0.0794], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0199],\n","        [ 0.4770, -2.2861]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0036168622318655252\n","\n","Initial state: tensor([ 0.0985, -0.0540], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0138], requires_grad=True)\n","New state: tensor([ 0.0984, -0.0550], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0137], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0540, -0.9444], grad_fn=<StackBackward0>) tensor([-0.0540, -0.9444], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0005,  0.0725], grad_fn=<StackBackward0>) tensor([-0.0005,  0.0725], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0371],\n","        [ 0.4401, -2.0631]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0034445105120539665\n","\n","Initial state: tensor([ 0.0984, -0.0550], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0137], requires_grad=True)\n","New state: tensor([ 0.0984, -0.0559], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0137], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0550, -0.9435], grad_fn=<StackBackward0>) tensor([-0.0550, -0.9435], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0006,  0.0644], grad_fn=<StackBackward0>) tensor([-0.0006,  0.0644], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0442],\n","        [ 0.3913, -1.8407]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003271173918619752\n","\n","Initial state: tensor([ 0.0984, -0.0559], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0137], requires_grad=True)\n","New state: tensor([ 0.0983, -0.0569], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0136], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0559, -0.9426], grad_fn=<StackBackward0>) tensor([-0.0559, -0.9426], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0553], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0553], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0288],\n","        [ 0.3318, -1.6189]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003097946522757411\n","\n","Initial state: tensor([ 0.0983, -0.0569], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0136], requires_grad=True)\n","New state: tensor([ 0.0983, -0.0578], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0136], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0569, -0.9417], grad_fn=<StackBackward0>) tensor([-0.0569, -0.9417], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-8.7500e-05,  4.5326e-02], grad_fn=<StackBackward0>) tensor([-8.7500e-05,  4.5326e-02], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0064],\n","        [ 0.2631, -1.3974]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0029256227426230907\n","\n","Initial state: tensor([ 0.0983, -0.0578], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0136], requires_grad=True)\n","New state: tensor([ 0.0982, -0.0588], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0578, -0.9407], grad_fn=<StackBackward0>) tensor([-0.0578, -0.9407], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-9.5215e-05,  3.4673e-02], grad_fn=<StackBackward0>) tensor([-9.5215e-05,  3.4673e-02], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0070],\n","        [ 0.1871, -1.1764]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.002755463821813464\n","\n","Initial state: tensor([ 0.0982, -0.0588], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0981, -0.0597], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0588, -0.9398], grad_fn=<StackBackward0>) tensor([-0.0588, -0.9398], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003,  0.0236], grad_fn=<StackBackward0>) tensor([-0.0003,  0.0236], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0251],\n","        [ 0.1066, -0.9556]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0025894392747431993\n","\n","Initial state: tensor([ 0.0981, -0.0597], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0981, -0.0606], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0597, -0.9388], grad_fn=<StackBackward0>) tensor([-0.0597, -0.9388], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0125], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0125], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0131],\n","        [ 0.0252, -0.7350]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.002432463224977255\n","\n","Initial state: tensor([ 0.0981, -0.0606], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0980, -0.0616], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0606, -0.9379], grad_fn=<StackBackward0>) tensor([-0.0606, -0.9379], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([9.9756e-05, 1.8359e-03], grad_fn=<StackBackward0>) tensor([9.9756e-05, 1.8359e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0074],\n","        [-0.0512, -0.5145]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0022941725328564644\n","\n","Initial state: tensor([ 0.0980, -0.0616], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0980, -0.0625], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0616, -0.9369], grad_fn=<StackBackward0>) tensor([-0.0616, -0.9369], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0005, -0.0073], grad_fn=<StackBackward0>) tensor([-0.0005, -0.0073], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0342],\n","        [-0.1129, -0.2942]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.002214641310274601\n","\n","Initial state: tensor([ 0.0980, -0.0625], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0979, -0.0634], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0625, -0.9359], grad_fn=<StackBackward0>) tensor([-0.0625, -0.9359], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0009, -0.0131], grad_fn=<StackBackward0>) tensor([ 0.0009, -0.0131], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0671],\n","        [-0.1408, -0.0753]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.002285189460963011\n","\n","Initial state: tensor([ 0.0979, -0.0634], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0978, -0.0644], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0634, -0.9349], grad_fn=<StackBackward0>) tensor([-0.0634, -0.9349], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0008, -0.0124], grad_fn=<StackBackward0>) tensor([-0.0008, -0.0124], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0603],\n","        [-0.1062,  0.1327]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0024796142242848873\n","\n","Initial state: tensor([ 0.0978, -0.0644], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0978, -0.0653], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0644, -0.9339], grad_fn=<StackBackward0>) tensor([-0.0644, -0.9339], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0007, -0.0076], grad_fn=<StackBackward0>) tensor([-0.0007, -0.0076], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0536],\n","        [-0.0348,  0.3037]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.002672258298844099\n","\n","Initial state: tensor([ 0.0978, -0.0653], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0977, -0.0662], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0653, -0.9329], grad_fn=<StackBackward0>) tensor([-0.0653, -0.9329], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-9.5233e-06, -2.1173e-03], grad_fn=<StackBackward0>) tensor([-9.5233e-06, -2.1173e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0007],\n","        [0.0378, 0.4353]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00284461653791368\n","\n","Initial state: tensor([ 0.0977, -0.0662], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0976, -0.0672], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0662, -0.9319], grad_fn=<StackBackward0>) tensor([-0.0662, -0.9319], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0006, 0.0025], grad_fn=<StackBackward0>) tensor([0.0006, 0.0025], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0437],\n","        [ 0.0974,  0.5310]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0029851270373910666\n","\n","Initial state: tensor([ 0.0976, -0.0672], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0976, -0.0681], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0672, -0.9309], grad_fn=<StackBackward0>) tensor([-0.0672, -0.9309], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0008, 0.0058], grad_fn=<StackBackward0>) tensor([0.0008, 0.0058], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0554],\n","        [ 0.1386,  0.5950]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0030926461331546307\n","\n","Initial state: tensor([ 0.0976, -0.0681], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0975, -0.0690], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0681, -0.9298], grad_fn=<StackBackward0>) tensor([-0.0681, -0.9298], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0005, 0.0074], grad_fn=<StackBackward0>) tensor([0.0005, 0.0074], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0354],\n","        [ 0.1595,  0.6308]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0031719375401735306\n","\n","Initial state: tensor([ 0.0975, -0.0690], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0974, -0.0700], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0690, -0.9288], grad_fn=<StackBackward0>) tensor([-0.0690, -0.9288], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-3.4098e-05,  7.4095e-03], grad_fn=<StackBackward0>) tensor([-3.4098e-05,  7.4095e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0025],\n","        [0.1610, 0.6412]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0032275919802486897\n","\n","Initial state: tensor([ 0.0974, -0.0700], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0974, -0.0709], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0700, -0.9277], grad_fn=<StackBackward0>) tensor([-0.0700, -0.9277], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0005,  0.0060], grad_fn=<StackBackward0>) tensor([-0.0005,  0.0060], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0398],\n","        [0.1449, 0.6288]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0032616625539958477\n","\n","Initial state: tensor([ 0.0974, -0.0709], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0973, -0.0718], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0709, -0.9267], grad_fn=<StackBackward0>) tensor([-0.0709, -0.9267], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0008,  0.0034], grad_fn=<StackBackward0>) tensor([-0.0008,  0.0034], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0593],\n","        [0.1142, 0.5959]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0032758039887994528\n","\n","Initial state: tensor([ 0.0973, -0.0718], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0972, -0.0727], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0718, -0.9256], grad_fn=<StackBackward0>) tensor([-0.0718, -0.9256], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-7.0313e-04, -5.6072e-05], grad_fn=<StackBackward0>) tensor([-7.0313e-04, -5.6072e-05], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0520],\n","        [0.0731, 0.5446]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00327475368976593\n","\n","Initial state: tensor([ 0.0972, -0.0727], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0971, -0.0737], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0727, -0.9245], grad_fn=<StackBackward0>) tensor([-0.0727, -0.9245], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0038], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0038], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0218],\n","        [0.0263, 0.4764]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00326552870683372\n","\n","Initial state: tensor([ 0.0971, -0.0737], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0971, -0.0746], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0737, -0.9234], grad_fn=<StackBackward0>) tensor([-0.0737, -0.9234], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0002, -0.0073], grad_fn=<StackBackward0>) tensor([ 0.0002, -0.0073], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0151],\n","        [-0.0201,  0.3929]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003250880166888237\n","\n","Initial state: tensor([ 0.0971, -0.0746], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0970, -0.0755], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0746, -0.9223], grad_fn=<StackBackward0>) tensor([-0.0746, -0.9223], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0004, -0.0098], grad_fn=<StackBackward0>) tensor([ 0.0004, -0.0098], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0329],\n","        [-0.0584,  0.2957]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0032272173557430506\n","\n","Initial state: tensor([ 0.0970, -0.0755], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0969, -0.0764], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0755, -0.9212], grad_fn=<StackBackward0>) tensor([-0.0755, -0.9212], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0002, -0.0104], grad_fn=<StackBackward0>) tensor([ 0.0002, -0.0104], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0130],\n","        [-0.0789,  0.1866]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003201704705134034\n","\n","Initial state: tensor([ 0.0969, -0.0764], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0968, -0.0774], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0136], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0764, -0.9201], grad_fn=<StackBackward0>) tensor([-0.0764, -0.9201], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004, -0.0078], grad_fn=<StackBackward0>) tensor([-0.0004, -0.0078], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0276],\n","        [-0.0688,  0.0681]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0032353345304727554\n","\n","Initial state: tensor([ 0.0968, -0.0774], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0136], requires_grad=True)\n","New state: tensor([ 0.0968, -0.0783], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0136], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0774, -0.9190], grad_fn=<StackBackward0>) tensor([-0.0774, -0.9190], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0001, -0.0004], grad_fn=<StackBackward0>) tensor([ 0.0001, -0.0004], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0089],\n","        [-0.0117, -0.0540]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003410940058529377\n","\n","Initial state: tensor([ 0.0968, -0.0783], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0136], requires_grad=True)\n","New state: tensor([ 0.0967, -0.0792], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0783, -0.9179], grad_fn=<StackBackward0>) tensor([-0.0783, -0.9179], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-6.1220e-05,  7.3901e-03], grad_fn=<StackBackward0>) tensor([-6.1220e-05,  7.3901e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0045],\n","        [ 0.0547, -0.1423]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0035584273282438517\n","\n","Initial state: tensor([ 0.0967, -0.0792], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0966, -0.0801], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0792, -0.9167], grad_fn=<StackBackward0>) tensor([-0.0792, -0.9167], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0116], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0116], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0124],\n","        [ 0.0888, -0.2011]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0036700060591101646\n","\n","Initial state: tensor([ 0.0966, -0.0801], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0965, -0.0810], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0801, -0.9156], grad_fn=<StackBackward0>) tensor([-0.0801, -0.9156], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001,  0.0123], grad_fn=<StackBackward0>) tensor([-0.0001,  0.0123], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0082],\n","        [ 0.0911, -0.2337]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003752546152099967\n","\n","Initial state: tensor([ 0.0965, -0.0810], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0964, -0.0819], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0810, -0.9144], grad_fn=<StackBackward0>) tensor([-0.0810, -0.9144], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([9.5828e-06, 1.0041e-02], grad_fn=<StackBackward0>) tensor([9.5828e-06, 1.0041e-02], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0007],\n","        [ 0.0677, -0.2423]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003814262803643942\n","\n","Initial state: tensor([ 0.0964, -0.0819], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0964, -0.0828], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0819, -0.9133], grad_fn=<StackBackward0>) tensor([-0.0819, -0.9133], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([5.9296e-05, 5.8697e-03], grad_fn=<StackBackward0>) tensor([5.9296e-05, 5.8697e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0044],\n","        [ 0.0278, -0.2288]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003862755373120308\n","\n","Initial state: tensor([ 0.0964, -0.0828], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0963, -0.0838], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0828, -0.9121], grad_fn=<StackBackward0>) tensor([-0.0828, -0.9121], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-1.4621e-05,  9.5329e-04], grad_fn=<StackBackward0>) tensor([-1.4621e-05,  9.5329e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0011],\n","        [-0.0168, -0.1946]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003902898170053959\n","\n","Initial state: tensor([ 0.0963, -0.0838], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0962, -0.0847], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0838, -0.9109], grad_fn=<StackBackward0>) tensor([-0.0838, -0.9109], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001, -0.0032], grad_fn=<StackBackward0>) tensor([-0.0001, -0.0032], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0082],\n","        [-0.0510, -0.1418]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003931166138499975\n","\n","Initial state: tensor([ 0.0962, -0.0847], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0961, -0.0856], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0847, -0.9097], grad_fn=<StackBackward0>) tensor([-0.0847, -0.9097], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-4.7772e-05, -4.7975e-03], grad_fn=<StackBackward0>) tensor([-4.7772e-05, -4.7975e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0035],\n","        [-0.0580, -0.0736]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.003947149496525526\n","\n","Initial state: tensor([ 0.0961, -0.0856], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0960, -0.0865], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0856, -0.9085], grad_fn=<StackBackward0>) tensor([-0.0856, -0.9085], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 9.8714e-05, -2.2078e-03], grad_fn=<StackBackward0>) tensor([ 9.8714e-05, -2.2078e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0073],\n","        [-0.0214,  0.0053]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004130235873162746\n","\n","Initial state: tensor([ 0.0960, -0.0865], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0959, -0.0874], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0865, -0.9073], grad_fn=<StackBackward0>) tensor([-0.0865, -0.9073], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0009,  0.0061], grad_fn=<StackBackward0>) tensor([-0.0009,  0.0061], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0653],\n","        [0.0705, 0.0716]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004262847825884819\n","\n","Initial state: tensor([ 0.0959, -0.0874], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0958, -0.0883], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0874, -0.9061], grad_fn=<StackBackward0>) tensor([-0.0874, -0.9061], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001,  0.0098], grad_fn=<StackBackward0>) tensor([-0.0001,  0.0098], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0106],\n","        [0.1138, 0.1180]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004374904558062553\n","\n","Initial state: tensor([ 0.0958, -0.0883], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0958, -0.0892], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0883, -0.9049], grad_fn=<StackBackward0>) tensor([-0.0883, -0.9049], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0007, 0.0089], grad_fn=<StackBackward0>) tensor([0.0007, 0.0089], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0493],\n","        [ 0.1081,  0.1438]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0044494736939668655\n","\n","Initial state: tensor([ 0.0958, -0.0892], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0957, -0.0901], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0892, -0.9036], grad_fn=<StackBackward0>) tensor([-0.0892, -0.9036], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0006, 0.0047], grad_fn=<StackBackward0>) tensor([0.0006, 0.0047], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0464],\n","        [ 0.0668,  0.1498]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004505757708102465\n","\n","Initial state: tensor([ 0.0957, -0.0901], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0956, -0.0910], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0901, -0.9024], grad_fn=<StackBackward0>) tensor([-0.0901, -0.9024], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001, -0.0013], grad_fn=<StackBackward0>) tensor([-0.0001, -0.0013], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0105],\n","        [0.0053, 0.1354]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004584700334817171\n","\n","Initial state: tensor([ 0.0956, -0.0910], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0955, -0.0919], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0910, -0.9011], grad_fn=<StackBackward0>) tensor([-0.0910, -0.9011], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0007, -0.0066], grad_fn=<StackBackward0>) tensor([-0.0007, -0.0066], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0497],\n","        [-0.0528,  0.1001]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004647566471248865\n","\n","Initial state: tensor([ 0.0955, -0.0919], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0954, -0.0928], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0919, -0.8999], grad_fn=<StackBackward0>) tensor([-0.0919, -0.8999], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002, -0.0085], grad_fn=<StackBackward0>) tensor([-0.0002, -0.0085], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0133],\n","        [-0.0783,  0.0501]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004713780712336302\n","\n","Initial state: tensor([ 0.0954, -0.0928], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0953, -0.0937], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0928, -0.8986], grad_fn=<StackBackward0>) tensor([-0.0928, -0.8986], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0006, -0.0049], grad_fn=<StackBackward0>) tensor([ 0.0006, -0.0049], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0423],\n","        [-0.0500, -0.0070]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004804031923413277\n","\n","Initial state: tensor([ 0.0953, -0.0937], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0952, -0.0946], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0937, -0.8973], grad_fn=<StackBackward0>) tensor([-0.0937, -0.8973], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0027], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0027], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0156],\n","        [ 0.0199, -0.0563]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.004944202955812216\n","\n","Initial state: tensor([ 0.0952, -0.0946], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0951, -0.0955], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0946, -0.8961], grad_fn=<StackBackward0>) tensor([-0.0946, -0.8961], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0074], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0074], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0308],\n","        [ 0.0637, -0.0801]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005042044445872307\n","\n","Initial state: tensor([ 0.0951, -0.0955], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0950, -0.0964], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0955, -0.8948], grad_fn=<StackBackward0>) tensor([-0.0955, -0.8948], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-4.3068e-05,  7.9047e-03], grad_fn=<StackBackward0>) tensor([-4.3068e-05,  7.9047e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0032],\n","        [ 0.0677, -0.0845]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005113949533551931\n","\n","Initial state: tensor([ 0.0950, -0.0964], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0949, -0.0973], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0964, -0.8935], grad_fn=<StackBackward0>) tensor([-0.0964, -0.8935], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0003, 0.0045], grad_fn=<StackBackward0>) tensor([0.0003, 0.0045], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0241],\n","        [ 0.0350, -0.0709]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005173087120056152\n","\n","Initial state: tensor([ 0.0949, -0.0973], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0948, -0.0982], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0973, -0.8922], grad_fn=<StackBackward0>) tensor([-0.0973, -0.8922], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 4.4316e-05, -1.3534e-03], grad_fn=<StackBackward0>) tensor([ 4.4316e-05, -1.3534e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0033],\n","        [-0.0188, -0.0392]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005280149634927511\n","\n","Initial state: tensor([ 0.0948, -0.0982], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0947, -0.0991], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0982, -0.8909], grad_fn=<StackBackward0>) tensor([-0.0982, -0.8909], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004, -0.0044], grad_fn=<StackBackward0>) tensor([-0.0004, -0.0044], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0286],\n","        [-0.0422,  0.0098]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005384108051657677\n","\n","Initial state: tensor([ 0.0947, -0.0991], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0946, -0.1000], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.0991, -0.8895], grad_fn=<StackBackward0>) tensor([-0.0991, -0.8895], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0003, -0.0024], grad_fn=<StackBackward0>) tensor([ 0.0003, -0.0024], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0186],\n","        [-0.0170,  0.0497]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0054999166168272495\n","\n","Initial state: tensor([ 0.0946, -0.1000], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0945, -0.1008], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1000, -0.8882], grad_fn=<StackBackward0>) tensor([-0.1000, -0.8882], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0014], grad_fn=<StackBackward0>) tensor([0.0002, 0.0014], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0134],\n","        [ 0.0230,  0.0653]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00560422521084547\n","\n","Initial state: tensor([ 0.0945, -0.1008], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0944, -0.1017], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1008, -0.8869], grad_fn=<StackBackward0>) tensor([-0.1008, -0.8869], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003,  0.0033], grad_fn=<StackBackward0>) tensor([-0.0003,  0.0033], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0186],\n","        [0.0405, 0.0582]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005678039044141769\n","\n","Initial state: tensor([ 0.0944, -0.1017], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0943, -0.1026], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1017, -0.8855], grad_fn=<StackBackward0>) tensor([-0.1017, -0.8855], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0021], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0021], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0152],\n","        [0.0252, 0.0337]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0057741315104067326\n","\n","Initial state: tensor([ 0.0943, -0.1026], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0942, -0.1035], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1026, -0.8842], grad_fn=<StackBackward0>) tensor([-0.1026, -0.8842], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0004, -0.0019], grad_fn=<StackBackward0>) tensor([ 0.0004, -0.0019], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0305],\n","        [-0.0200, -0.0056]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.005898372735828161\n","\n","Initial state: tensor([ 0.0942, -0.1035], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0941, -0.1044], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1035, -0.8828], grad_fn=<StackBackward0>) tensor([-0.1035, -0.8828], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004, -0.0025], grad_fn=<StackBackward0>) tensor([-0.0004, -0.0025], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0304],\n","        [-0.0303, -0.0377]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0060044690035283566\n","\n","Initial state: tensor([ 0.0941, -0.1044], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0940, -0.1053], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1044, -0.8814], grad_fn=<StackBackward0>) tensor([-0.1044, -0.8814], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0003], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0003], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0254],\n","        [-0.0103, -0.0514]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00610598037019372\n","\n","Initial state: tensor([ 0.0940, -0.1053], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0939, -0.1061], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1053, -0.8800], grad_fn=<StackBackward0>) tensor([-0.1053, -0.8800], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0003, 0.0023], grad_fn=<StackBackward0>) tensor([0.0003, 0.0023], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0252],\n","        [ 0.0175, -0.0436]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00619495240971446\n","\n","Initial state: tensor([ 0.0939, -0.1061], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0938, -0.1070], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1061, -0.8786], grad_fn=<StackBackward0>) tensor([-0.1061, -0.8786], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0027], grad_fn=<StackBackward0>) tensor([0.0002, 0.0027], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0177],\n","        [ 0.0244, -0.0177]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.006313864607363939\n","\n","Initial state: tensor([ 0.0938, -0.1070], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0937, -0.1079], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1070, -0.8772], grad_fn=<StackBackward0>) tensor([-0.1070, -0.8772], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0006, -0.0010], grad_fn=<StackBackward0>) tensor([-0.0006, -0.0010], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0425],\n","        [-0.0077,  0.0173]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.006425376050174236\n","\n","Initial state: tensor([ 0.0937, -0.1079], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0936, -0.1088], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1079, -0.8758], grad_fn=<StackBackward0>) tensor([-0.1079, -0.8758], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-9.3912e-05, -3.3013e-03], grad_fn=<StackBackward0>) tensor([-9.3912e-05, -3.3013e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0070],\n","        [-0.0276,  0.0403]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.006544370669871569\n","\n","Initial state: tensor([ 0.0936, -0.1088], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0935, -0.1096], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1088, -0.8744], grad_fn=<StackBackward0>) tensor([-0.1088, -0.8744], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0005, -0.0021], grad_fn=<StackBackward0>) tensor([ 0.0005, -0.0021], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0372],\n","        [-0.0149,  0.0421]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.006629831623286009\n","\n","Initial state: tensor([ 0.0935, -0.1096], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0934, -0.1105], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1096, -0.8730], grad_fn=<StackBackward0>) tensor([-0.1096, -0.8730], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0007], grad_fn=<StackBackward0>) tensor([0.0002, 0.0007], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0157],\n","        [ 0.0105,  0.0269]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0067582810297608376\n","\n","Initial state: tensor([ 0.0934, -0.1105], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0932, -0.1114], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1105, -0.8716], grad_fn=<StackBackward0>) tensor([-0.1105, -0.8716], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0006,  0.0017], grad_fn=<StackBackward0>) tensor([-0.0006,  0.0017], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0476],\n","        [ 0.0159, -0.0058]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.006859752349555492\n","\n","Initial state: tensor([ 0.0932, -0.1114], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0931, -0.1123], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1114, -0.8701], grad_fn=<StackBackward0>) tensor([-0.1114, -0.8701], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003,  0.0008], grad_fn=<StackBackward0>) tensor([-0.0003,  0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0227],\n","        [ 0.0035, -0.0327]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0069936844520270824\n","\n","Initial state: tensor([ 0.0931, -0.1123], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0930, -0.1131], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1123, -0.8687], grad_fn=<StackBackward0>) tensor([-0.1123, -0.8687], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0006, -0.0007], grad_fn=<StackBackward0>) tensor([ 0.0006, -0.0007], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0450],\n","        [-0.0125, -0.0379]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007101801224052906\n","\n","Initial state: tensor([ 0.0930, -0.1131], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0929, -0.1140], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1131, -0.8672], grad_fn=<StackBackward0>) tensor([-0.1131, -0.8672], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0006, -0.0012], grad_fn=<StackBackward0>) tensor([ 0.0006, -0.0012], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0431],\n","        [-0.0155, -0.0278]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007184931077063084\n","\n","Initial state: tensor([ 0.0929, -0.1140], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0928, -0.1149], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1140, -0.8658], grad_fn=<StackBackward0>) tensor([-0.1140, -0.8658], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0001], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0001], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0210],\n","        [-0.0022, -0.0066]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007304562255740166\n","\n","Initial state: tensor([ 0.0928, -0.1149], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0927, -0.1157], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1149, -0.8643], grad_fn=<StackBackward0>) tensor([-0.1149, -0.8643], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001,  0.0013], grad_fn=<StackBackward0>) tensor([-0.0001,  0.0013], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0078],\n","        [0.0153, 0.0196]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007431107573211193\n","\n","Initial state: tensor([ 0.0927, -0.1157], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0926, -0.1166], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1157, -0.8628], grad_fn=<StackBackward0>) tensor([-0.1157, -0.8628], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0003, -0.0005], grad_fn=<StackBackward0>) tensor([ 0.0003, -0.0005], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0249],\n","        [-0.0015,  0.0254]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007533211261034012\n","\n","Initial state: tensor([ 0.0926, -0.1166], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0924, -0.1174], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1166, -0.8614], grad_fn=<StackBackward0>) tensor([-0.1166, -0.8614], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 4.0353e-05, -1.6212e-03], grad_fn=<StackBackward0>) tensor([ 4.0353e-05, -1.6212e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0030],\n","        [-0.0144,  0.0138]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007664783392101526\n","\n","Initial state: tensor([ 0.0924, -0.1174], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0923, -0.1183], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1174, -0.8599], grad_fn=<StackBackward0>) tensor([-0.1174, -0.8599], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0015], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0015], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0283],\n","        [ 0.0131, -0.0128]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00778278848156333\n","\n","Initial state: tensor([ 0.0923, -0.1183], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0922, -0.1192], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1183, -0.8584], grad_fn=<StackBackward0>) tensor([-0.1183, -0.8584], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([1.9494e-05, 2.0316e-03], grad_fn=<StackBackward0>) tensor([1.9494e-05, 2.0316e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0014],\n","        [ 0.0166, -0.0279]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.007903958670794964\n","\n","Initial state: tensor([ 0.0922, -0.1192], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0921, -0.1200], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1192, -0.8569], grad_fn=<StackBackward0>) tensor([-0.1192, -0.8569], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0003, -0.0005], grad_fn=<StackBackward0>) tensor([ 0.0003, -0.0005], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0243],\n","        [-0.0081, -0.0212]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008009709417819977\n","\n","Initial state: tensor([ 0.0921, -0.1200], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0920, -0.1209], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1200, -0.8553], grad_fn=<StackBackward0>) tensor([-0.1200, -0.8553], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-8.2501e-05, -1.7044e-03], grad_fn=<StackBackward0>) tensor([-8.2501e-05, -1.7044e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000e+00,  6.1153e-03],\n","        [-1.7056e-02,  7.6294e-06]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008147001266479492\n","\n","Initial state: tensor([ 0.0920, -0.1209], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0918, -0.1217], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1209, -0.8538], grad_fn=<StackBackward0>) tensor([-0.1209, -0.8538], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0023], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0023], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0115],\n","        [0.0253, 0.0191]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008269200101494789\n","\n","Initial state: tensor([ 0.0918, -0.1217], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0917, -0.1226], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1217, -0.8523], grad_fn=<StackBackward0>) tensor([-0.1217, -0.8523], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([7.8466e-05, 2.0328e-03], grad_fn=<StackBackward0>) tensor([7.8466e-05, 2.0328e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0058],\n","        [ 0.0234,  0.0228]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00837956927716732\n","\n","Initial state: tensor([ 0.0917, -0.1226], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0916, -0.1234], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1226, -0.8508], grad_fn=<StackBackward0>) tensor([-0.1226, -0.8508], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0001, -0.0016], grad_fn=<StackBackward0>) tensor([ 0.0001, -0.0016], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0086],\n","        [-0.0152,  0.0097]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.00850935373455286\n","\n","Initial state: tensor([ 0.0916, -0.1234], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0915, -0.1243], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1234, -0.8492], grad_fn=<StackBackward0>) tensor([-0.1234, -0.8492], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0008], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0190],\n","        [-0.0103, -0.0138]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008636043407022953\n","\n","Initial state: tensor([ 0.0915, -0.1243], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0913, -0.1251], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1243, -0.8476], grad_fn=<StackBackward0>) tensor([-0.1243, -0.8476], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([6.5318e-05, 1.7780e-03], grad_fn=<StackBackward0>) tensor([6.5318e-05, 1.7780e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0048],\n","        [ 0.0148, -0.0221]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008754336275160313\n","\n","Initial state: tensor([ 0.0913, -0.1251], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0912, -0.1260], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1251, -0.8461], grad_fn=<StackBackward0>) tensor([-0.1251, -0.8461], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0010], grad_fn=<StackBackward0>) tensor([0.0002, 0.0010], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0135],\n","        [ 0.0091, -0.0099]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.008897588588297367\n","\n","Initial state: tensor([ 0.0912, -0.1260], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0911, -0.1268], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1260, -0.8445], grad_fn=<StackBackward0>) tensor([-0.1260, -0.8445], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004, -0.0022], grad_fn=<StackBackward0>) tensor([-0.0004, -0.0022], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0260],\n","        [-0.0205,  0.0136]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009024018421769142\n","\n","Initial state: tensor([ 0.0911, -0.1268], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0910, -0.1276], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1268, -0.8429], grad_fn=<StackBackward0>) tensor([-0.1268, -0.8429], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002, -0.0021], grad_fn=<StackBackward0>) tensor([-0.0002, -0.0021], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0152],\n","        [-0.0177,  0.0256]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009148732759058475\n","\n","Initial state: tensor([ 0.0910, -0.1276], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0908, -0.1285], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1276, -0.8414], grad_fn=<StackBackward0>) tensor([-0.1276, -0.8414], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0003, 0.0008], grad_fn=<StackBackward0>) tensor([0.0003, 0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0216],\n","        [ 0.0109,  0.0185]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009268383495509624\n","\n","Initial state: tensor([ 0.0908, -0.1285], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0907, -0.1293], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1285, -0.8398], grad_fn=<StackBackward0>) tensor([-0.1285, -0.8398], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0001, 0.0019], grad_fn=<StackBackward0>) tensor([0.0001, 0.0019], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0088],\n","        [ 0.0184, -0.0026]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009418492205440998\n","\n","Initial state: tensor([ 0.0907, -0.1293], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0906, -0.1302], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1293, -0.8382], grad_fn=<StackBackward0>) tensor([-0.1293, -0.8382], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004, -0.0017], grad_fn=<StackBackward0>) tensor([-0.0004, -0.0017], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0304],\n","        [-0.0199, -0.0186]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009550624527037144\n","\n","Initial state: tensor([ 0.0906, -0.1302], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0905, -0.1310], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1302, -0.8366], grad_fn=<StackBackward0>) tensor([-0.1302, -0.8366], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0027], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0027], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0199],\n","        [-0.0299, -0.0219]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009662529453635216\n","\n","Initial state: tensor([ 0.0905, -0.1310], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0903, -0.1318], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1310, -0.8349], grad_fn=<StackBackward0>) tensor([-0.1310, -0.8349], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0002, -0.0002], grad_fn=<StackBackward0>) tensor([ 0.0002, -0.0002], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0177],\n","        [-0.0035, -0.0122]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009803666733205318\n","\n","Initial state: tensor([ 0.0903, -0.1318], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0902, -0.1327], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1318, -0.8333], grad_fn=<StackBackward0>) tensor([-0.1318, -0.8333], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([1.5198e-05, 2.6887e-03], grad_fn=<StackBackward0>) tensor([1.5198e-05, 2.6887e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0011],\n","        [ 0.0283,  0.0103]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.009936672635376453\n","\n","Initial state: tensor([ 0.0902, -0.1327], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0901, -0.1335], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1327, -0.8317], grad_fn=<StackBackward0>) tensor([-0.1327, -0.8317], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0007], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0007], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0169],\n","        [0.0103, 0.0222]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.010068561881780624\n","\n","Initial state: tensor([ 0.0901, -0.1335], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0899, -0.1343], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1335, -0.8300], grad_fn=<StackBackward0>) tensor([-0.1335, -0.8300], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 4.3149e-06, -2.4885e-03], grad_fn=<StackBackward0>) tensor([ 4.3149e-06, -2.4885e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0003],\n","        [-0.0229,  0.0145]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.010196110233664513\n","\n","Initial state: tensor([ 0.0899, -0.1343], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0898, -0.1352], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1343, -0.8284], grad_fn=<StackBackward0>) tensor([-0.1343, -0.8284], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0002, -0.0011], grad_fn=<StackBackward0>) tensor([ 0.0002, -0.0011], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0144],\n","        [-0.0115, -0.0056]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01034986600279808\n","\n","Initial state: tensor([ 0.0898, -0.1352], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0897, -0.1360], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1352, -0.8268], grad_fn=<StackBackward0>) tensor([-0.1352, -0.8268], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0030], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0030], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0184],\n","        [ 0.0277, -0.0166]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.010485921055078506\n","\n","Initial state: tensor([ 0.0897, -0.1360], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0895, -0.1368], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1360, -0.8251], grad_fn=<StackBackward0>) tensor([-0.1360, -0.8251], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003,  0.0029], grad_fn=<StackBackward0>) tensor([-0.0003,  0.0029], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0192],\n","        [ 0.0269, -0.0155]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.010601462796330452\n","\n","Initial state: tensor([ 0.0895, -0.1368], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0894, -0.1376], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1368, -0.8234], grad_fn=<StackBackward0>) tensor([-0.1368, -0.8234], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0001, -0.0008], grad_fn=<StackBackward0>) tensor([ 0.0001, -0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0102],\n","        [-0.0089, -0.0040]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.010744412429630756\n","\n","Initial state: tensor([ 0.0894, -0.1376], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0892, -0.1384], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1376, -0.8217], grad_fn=<StackBackward0>) tensor([-0.1376, -0.8217], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-8.7527e-05, -1.2912e-03], grad_fn=<StackBackward0>) tensor([-8.7527e-05, -1.2912e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0065],\n","        [-0.0111,  0.0134]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0108840586617589\n","\n","Initial state: tensor([ 0.0892, -0.1384], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0891, -0.1393], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1384, -0.8201], grad_fn=<StackBackward0>) tensor([-0.1384, -0.8201], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-2.7143e-05,  1.3647e-03], grad_fn=<StackBackward0>) tensor([-2.7143e-05,  1.3647e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0020],\n","        [0.0151, 0.0110]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01101501751691103\n","\n","Initial state: tensor([ 0.0891, -0.1393], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0890, -0.1401], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1393, -0.8184], grad_fn=<StackBackward0>) tensor([-0.1393, -0.8184], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0001, 0.0002], grad_fn=<StackBackward0>) tensor([0.0001, 0.0002], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0075],\n","        [ 0.0009, -0.0057]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0111811188980937\n","\n","Initial state: tensor([ 0.0890, -0.1401], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0888, -0.1409], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1401, -0.8167], grad_fn=<StackBackward0>) tensor([-0.1401, -0.8167], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004, -0.0016], grad_fn=<StackBackward0>) tensor([-0.0004, -0.0016], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0302],\n","        [-0.0165, -0.0057]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.011306339874863625\n","\n","Initial state: tensor([ 0.0888, -0.1409], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0887, -0.1417], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1409, -0.8150], grad_fn=<StackBackward0>) tensor([-0.1409, -0.8150], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002, -0.0010], grad_fn=<StackBackward0>) tensor([-0.0002, -0.0010], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0154],\n","        [-0.0100, -0.0016]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.011479636654257774\n","\n","Initial state: tensor([ 0.0887, -0.1417], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0885, -0.1425], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1417, -0.8133], grad_fn=<StackBackward0>) tensor([-0.1417, -0.8133], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0006, 0.0021], grad_fn=<StackBackward0>) tensor([0.0006, 0.0021], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0434],\n","        [ 0.0212,  0.0043]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01162983849644661\n","\n","Initial state: tensor([ 0.0885, -0.1425], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0884, -0.1433], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1425, -0.8115], grad_fn=<StackBackward0>) tensor([-0.1425, -0.8115], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0006, 0.0028], grad_fn=<StackBackward0>) tensor([0.0006, 0.0028], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0458],\n","        [ 0.0288,  0.0073]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.011732577346265316\n","\n","Initial state: tensor([ 0.0884, -0.1433], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0883, -0.1441], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1433, -0.8098], grad_fn=<StackBackward0>) tensor([-0.1433, -0.8098], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([2.6611e-05, 1.0084e-03], grad_fn=<StackBackward0>) tensor([2.6611e-05, 1.0084e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0020],\n","        [ 0.0110,  0.0066]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.011928517371416092\n","\n","Initial state: tensor([ 0.0883, -0.1441], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0881, -0.1450], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1441, -0.8081], grad_fn=<StackBackward0>) tensor([-0.1441, -0.8081], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0006, -0.0043], grad_fn=<StackBackward0>) tensor([-0.0006, -0.0043], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0465],\n","        [-0.0441, -0.0070]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.012091705575585365\n","\n","Initial state: tensor([ 0.0881, -0.1450], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0880, -0.1458], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1450, -0.8063], grad_fn=<StackBackward0>) tensor([-0.1450, -0.8063], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0007, -0.0060], grad_fn=<StackBackward0>) tensor([-0.0007, -0.0060], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0518],\n","        [-0.0619, -0.0165]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.012209184467792511\n","\n","Initial state: tensor([ 0.0880, -0.1458], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0878, -0.1466], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1458, -0.8046], grad_fn=<StackBackward0>) tensor([-0.1458, -0.8046], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0041], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0041], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0239],\n","        [-0.0435, -0.0200]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.012333089485764503\n","\n","Initial state: tensor([ 0.0878, -0.1466], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0877, -0.1474], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1466, -0.8028], grad_fn=<StackBackward0>) tensor([-0.1466, -0.8028], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0003, 0.0012], grad_fn=<StackBackward0>) tensor([0.0003, 0.0012], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0243],\n","        [ 0.0099, -0.0136]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0125010060146451\n","\n","Initial state: tensor([ 0.0877, -0.1474], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0875, -0.1482], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1474, -0.8011], grad_fn=<StackBackward0>) tensor([-0.1474, -0.8011], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0003, 0.0042], grad_fn=<StackBackward0>) tensor([0.0003, 0.0042], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0232],\n","        [ 0.0428,  0.0040]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01263858936727047\n","\n","Initial state: tensor([ 0.0875, -0.1482], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0874, -0.1490], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1482, -0.7993], grad_fn=<StackBackward0>) tensor([-0.1482, -0.7993], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-4.5749e-05,  3.0204e-03], grad_fn=<StackBackward0>) tensor([-4.5749e-05,  3.0204e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0034],\n","        [0.0326, 0.0178]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.012784100137650967\n","\n","Initial state: tensor([ 0.0874, -0.1490], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0872, -0.1498], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1490, -0.7975], grad_fn=<StackBackward0>) tensor([-0.1490, -0.7975], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0018], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0018], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0233],\n","        [-0.0154,  0.0179]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.012940089218318462\n","\n","Initial state: tensor([ 0.0872, -0.1498], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0871, -0.1506], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1498, -0.7957], grad_fn=<StackBackward0>) tensor([-0.1498, -0.7957], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-6.9985e-05, -3.8462e-03], grad_fn=<StackBackward0>) tensor([-6.9985e-05, -3.8462e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0052],\n","        [-0.0380,  0.0040]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01307873334735632\n","\n","Initial state: tensor([ 0.0871, -0.1506], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0869, -0.1513], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1506, -0.7940], grad_fn=<StackBackward0>) tensor([-0.1506, -0.7940], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0002, -0.0013], grad_fn=<StackBackward0>) tensor([ 0.0002, -0.0013], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0175],\n","        [-0.0146, -0.0112]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.013240101747214794\n","\n","Initial state: tensor([ 0.0869, -0.1513], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0868, -0.1521], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1513, -0.7922], grad_fn=<StackBackward0>) tensor([-0.1513, -0.7922], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([1.7236e-05, 3.3442e-03], grad_fn=<StackBackward0>) tensor([1.7236e-05, 3.3442e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0013],\n","        [ 0.0316, -0.0135]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0133954593911767\n","\n","Initial state: tensor([ 0.0868, -0.1521], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0866, -0.1529], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1521, -0.7904], grad_fn=<StackBackward0>) tensor([-0.1521, -0.7904], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0033], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0033], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0160],\n","        [ 0.0328, -0.0053]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.013524510897696018\n","\n","Initial state: tensor([ 0.0866, -0.1529], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0865, -0.1537], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1529, -0.7885], grad_fn=<StackBackward0>) tensor([-0.1529, -0.7885], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001, -0.0005], grad_fn=<StackBackward0>) tensor([-0.0001, -0.0005], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0097],\n","        [-0.0046,  0.0058]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.013710086233913898\n","\n","Initial state: tensor([ 0.0865, -0.1537], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0863, -0.1545], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1537, -0.7867], grad_fn=<StackBackward0>) tensor([-0.1537, -0.7867], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0005, -0.0022], grad_fn=<StackBackward0>) tensor([ 0.0005, -0.0022], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0363],\n","        [-0.0220,  0.0033]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.013859899714589119\n","\n","Initial state: tensor([ 0.0863, -0.1545], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0862, -0.1553], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1545, -0.7849], grad_fn=<StackBackward0>) tensor([-0.1545, -0.7849], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0004, -0.0015], grad_fn=<StackBackward0>) tensor([ 0.0004, -0.0015], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0330],\n","        [-0.0154, -0.0010]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.013998232781887054\n","\n","Initial state: tensor([ 0.0862, -0.1553], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0860, -0.1561], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1553, -0.7831], grad_fn=<StackBackward0>) tensor([-0.1553, -0.7831], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0009], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0009], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0160],\n","        [ 0.0088, -0.0042]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01415698230266571\n","\n","Initial state: tensor([ 0.0860, -0.1561], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0858, -0.1568], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1561, -0.7812], grad_fn=<StackBackward0>) tensor([-0.1561, -0.7812], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0010], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0010], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0179],\n","        [ 0.0103, -0.0012]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.014318938367068768\n","\n","Initial state: tensor([ 0.0858, -0.1568], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0857, -0.1576], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1568, -0.7794], grad_fn=<StackBackward0>) tensor([-0.1568, -0.7794], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0003, -0.0010], grad_fn=<StackBackward0>) tensor([ 0.0003, -0.0010], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0223],\n","        [-0.0099,  0.0030]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01447055023163557\n","\n","Initial state: tensor([ 0.0857, -0.1576], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0855, -0.1584], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1576, -0.7775], grad_fn=<StackBackward0>) tensor([-0.1576, -0.7775], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0002, -0.0011], grad_fn=<StackBackward0>) tensor([ 0.0002, -0.0011], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0133],\n","        [-0.0106,  0.0035]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.014647694304585457\n","\n","Initial state: tensor([ 0.0855, -0.1584], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0854, -0.1592], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1584, -0.7757], grad_fn=<StackBackward0>) tensor([-0.1584, -0.7757], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0015], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0015], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0322],\n","        [ 0.0150, -0.0014]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.014808371663093567\n","\n","Initial state: tensor([ 0.0854, -0.1592], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0852, -0.1600], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1592, -0.7738], grad_fn=<StackBackward0>) tensor([-0.1592, -0.7738], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0021], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0021], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0302],\n","        [ 0.0199, -0.0048]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.014943857677280903\n","\n","Initial state: tensor([ 0.0852, -0.1600], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0850, -0.1607], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1600, -0.7719], grad_fn=<StackBackward0>) tensor([-0.1600, -0.7719], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0001, 0.0001], grad_fn=<StackBackward0>) tensor([0.0001, 0.0001], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0100],\n","        [ 0.0008, -0.0044]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.015115500427782536\n","\n","Initial state: tensor([ 0.0850, -0.1607], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0849, -0.1615], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1607, -0.7700], grad_fn=<StackBackward0>) tensor([-0.1607, -0.7700], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 3.3688e-05, -2.0495e-03], grad_fn=<StackBackward0>) tensor([ 3.3688e-05, -2.0495e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0025],\n","        [-0.0196,  0.0067]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.015270414762198925\n","\n","Initial state: tensor([ 0.0849, -0.1615], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0847, -0.1623], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1615, -0.7681], grad_fn=<StackBackward0>) tensor([-0.1615, -0.7681], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001,  0.0001], grad_fn=<StackBackward0>) tensor([-0.0001,  0.0001], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0105],\n","        [0.0024, 0.0081]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.015441423282027245\n","\n","Initial state: tensor([ 0.0847, -0.1623], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0846, -0.1630], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1623, -0.7663], grad_fn=<StackBackward0>) tensor([-0.1623, -0.7663], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0016], grad_fn=<StackBackward0>) tensor([0.0002, 0.0016], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0136],\n","        [ 0.0146, -0.0069]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01559754554182291\n","\n","Initial state: tensor([ 0.0846, -0.1630], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0844, -0.1638], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1630, -0.7644], grad_fn=<StackBackward0>) tensor([-0.1630, -0.7644], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 6.3257e-05, -2.5477e-04], grad_fn=<StackBackward0>) tensor([ 6.3257e-05, -2.5477e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0047],\n","        [-0.0041, -0.0116]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.015772145241498947\n","\n","Initial state: tensor([ 0.0844, -0.1638], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0842, -0.1645], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1638, -0.7624], grad_fn=<StackBackward0>) tensor([-0.1638, -0.7624], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0009], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0009], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0205],\n","        [-0.0076,  0.0080]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01593128591775894\n","\n","Initial state: tensor([ 0.0842, -0.1645], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0841, -0.1653], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1645, -0.7605], grad_fn=<StackBackward0>) tensor([-0.1645, -0.7605], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-4.1368e-05,  1.0647e-04], grad_fn=<StackBackward0>) tensor([-4.1368e-05,  1.0647e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0031],\n","        [0.0033, 0.0163]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.016101930290460587\n","\n","Initial state: tensor([ 0.0841, -0.1653], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0839, -0.1661], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1653, -0.7586], grad_fn=<StackBackward0>) tensor([-0.1653, -0.7586], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0003, 0.0005], grad_fn=<StackBackward0>) tensor([0.0003, 0.0005], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0204],\n","        [ 0.0047, -0.0023]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01626484841108322\n","\n","Initial state: tensor([ 0.0839, -0.1661], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0837, -0.1668], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1661, -0.7567], grad_fn=<StackBackward0>) tensor([-0.1661, -0.7567], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-4.7594e-05, -1.4722e-04], grad_fn=<StackBackward0>) tensor([-4.7594e-05, -1.4722e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0035],\n","        [-0.0037, -0.0162]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.016432011500000954\n","\n","Initial state: tensor([ 0.0837, -0.1668], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0836, -0.1676], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1668, -0.7547], grad_fn=<StackBackward0>) tensor([-0.1668, -0.7547], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002, -0.0002], grad_fn=<StackBackward0>) tensor([-0.0002, -0.0002], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0162],\n","        [-0.0019, -0.0028]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.016605960205197334\n","\n","Initial state: tensor([ 0.0836, -0.1676], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0834, -0.1683], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1676, -0.7528], grad_fn=<StackBackward0>) tensor([-0.1676, -0.7528], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0003], grad_fn=<StackBackward0>) tensor([0.0002, 0.0003], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0161],\n","        [ 0.0047,  0.0138]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.016769731417298317\n","\n","Initial state: tensor([ 0.0834, -0.1683], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0832, -0.1691], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1683, -0.7509], grad_fn=<StackBackward0>) tensor([-0.1683, -0.7509], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 1.5038e-04, -2.9125e-05], grad_fn=<StackBackward0>) tensor([ 1.5038e-04, -2.9125e-05], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0111],\n","        [ 0.0013,  0.0115]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.016950739547610283\n","\n","Initial state: tensor([ 0.0832, -0.1691], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0831, -0.1698], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1691, -0.7489], grad_fn=<StackBackward0>) tensor([-0.1691, -0.7489], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0004], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0004], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0249],\n","        [-0.0052, -0.0101]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01712019182741642\n","\n","Initial state: tensor([ 0.0831, -0.1698], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0829, -0.1706], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1698, -0.7469], grad_fn=<StackBackward0>) tensor([-0.1698, -0.7469], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-2.3402e-04, -1.2047e-05], grad_fn=<StackBackward0>) tensor([-2.3402e-04, -1.2047e-05], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0173],\n","        [-0.0028, -0.0196]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.017285775393247604\n","\n","Initial state: tensor([ 0.0829, -0.1706], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0827, -0.1713], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1706, -0.7450], grad_fn=<StackBackward0>) tensor([-0.1706, -0.7450], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0005], grad_fn=<StackBackward0>) tensor([0.0002, 0.0005], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0180],\n","        [ 0.0041, -0.0078]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01745462417602539\n","\n","Initial state: tensor([ 0.0827, -0.1713], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0826, -0.1721], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1713, -0.7430], grad_fn=<StackBackward0>) tensor([-0.1713, -0.7430], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 1.2984e-04, -4.6414e-05], grad_fn=<StackBackward0>) tensor([ 1.2984e-04, -4.6414e-05], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0096],\n","        [ 0.0013,  0.0134]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.017637355253100395\n","\n","Initial state: tensor([ 0.0826, -0.1721], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0824, -0.1728], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1721, -0.7410], grad_fn=<StackBackward0>) tensor([-0.1721, -0.7410], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0006], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0006], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0240],\n","        [-0.0047,  0.0103]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01780020073056221\n","\n","Initial state: tensor([ 0.0824, -0.1728], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0822, -0.1735], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1728, -0.7390], grad_fn=<StackBackward0>) tensor([-0.1728, -0.7390], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002, -0.0002], grad_fn=<StackBackward0>) tensor([-0.0002, -0.0002], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0154],\n","        [-0.0024, -0.0031]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01799473911523819\n","\n","Initial state: tensor([ 0.0822, -0.1735], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0820, -0.1743], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1735, -0.7370], grad_fn=<StackBackward0>) tensor([-0.1735, -0.7370], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0005, 0.0008], grad_fn=<StackBackward0>) tensor([0.0005, 0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0335],\n","        [ 0.0063, -0.0099]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01817268319427967\n","\n","Initial state: tensor([ 0.0820, -0.1743], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0819, -0.1750], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1743, -0.7350], grad_fn=<StackBackward0>) tensor([-0.1743, -0.7350], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0005, 0.0007], grad_fn=<StackBackward0>) tensor([0.0005, 0.0007], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0362],\n","        [ 0.0062, -0.0084]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01831386610865593\n","\n","Initial state: tensor([ 0.0819, -0.1750], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0817, -0.1757], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1750, -0.7330], grad_fn=<StackBackward0>) tensor([-0.1750, -0.7330], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-3.4229e-05, -1.0402e-04], grad_fn=<StackBackward0>) tensor([-3.4229e-05, -1.0402e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0025],\n","        [-0.0012, -0.0009]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.018505211919546127\n","\n","Initial state: tensor([ 0.0817, -0.1757], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0815, -0.1765], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1757, -0.7310], grad_fn=<StackBackward0>) tensor([-0.1757, -0.7310], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-2.1657e-05,  7.7951e-04], grad_fn=<StackBackward0>) tensor([-2.1657e-05,  7.7951e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0016],\n","        [0.0098, 0.0146]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.018673045560717583\n","\n","Initial state: tensor([ 0.0815, -0.1765], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0813, -0.1772], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1765, -0.7290], grad_fn=<StackBackward0>) tensor([-0.1765, -0.7290], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 3.3714e-05, -5.7933e-04], grad_fn=<StackBackward0>) tensor([ 3.3714e-05, -5.7933e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0025],\n","        [-0.0050,  0.0057]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01886819116771221\n","\n","Initial state: tensor([ 0.0813, -0.1772], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0812, -0.1779], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1772, -0.7270], grad_fn=<StackBackward0>) tensor([-0.1772, -0.7270], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001,  0.0012], grad_fn=<StackBackward0>) tensor([-0.0001,  0.0012], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0076],\n","        [ 0.0095, -0.0220]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.019045790657401085\n","\n","Initial state: tensor([ 0.0812, -0.1779], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0810, -0.1786], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1779, -0.7249], grad_fn=<StackBackward0>) tensor([-0.1779, -0.7249], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-6.0359e-05,  8.8691e-04], grad_fn=<StackBackward0>) tensor([-6.0359e-05,  8.8691e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0045],\n","        [ 0.0058, -0.0227]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.019212016835808754\n","\n","Initial state: tensor([ 0.0810, -0.1786], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0808, -0.1794], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1786, -0.7229], grad_fn=<StackBackward0>) tensor([-0.1786, -0.7229], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 7.6387e-05, -8.7076e-04], grad_fn=<StackBackward0>) tensor([ 7.6387e-05, -8.7076e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0057],\n","        [-0.0083,  0.0027]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.019406689330935478\n","\n","Initial state: tensor([ 0.0808, -0.1794], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0806, -0.1801], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1794, -0.7208], grad_fn=<StackBackward0>) tensor([-0.1794, -0.7208], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001,  0.0012], grad_fn=<StackBackward0>) tensor([-0.0001,  0.0012], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0087],\n","        [0.0143, 0.0185]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01957976631820202\n","\n","Initial state: tensor([ 0.0806, -0.1801], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0804, -0.1808], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1801, -0.7188], grad_fn=<StackBackward0>) tensor([-0.1801, -0.7188], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001,  0.0008], grad_fn=<StackBackward0>) tensor([-0.0001,  0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0074],\n","        [0.0093, 0.0122]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.01977064460515976\n","\n","Initial state: tensor([ 0.0804, -0.1808], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0803, -0.1815], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1808, -0.7167], grad_fn=<StackBackward0>) tensor([-0.1808, -0.7167], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0002, -0.0017], grad_fn=<StackBackward0>) tensor([ 0.0002, -0.0017], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0116],\n","        [-0.0192, -0.0132]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.019954200834035873\n","\n","Initial state: tensor([ 0.0803, -0.1815], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0801, -0.1822], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1815, -0.7147], grad_fn=<StackBackward0>) tensor([-0.1815, -0.7147], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0001, -0.0009], grad_fn=<StackBackward0>) tensor([ 0.0001, -0.0009], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0093],\n","        [-0.0124, -0.0221]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02012794278562069\n","\n","Initial state: tensor([ 0.0801, -0.1822], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0799, -0.1829], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1822, -0.7126], grad_fn=<StackBackward0>) tensor([-0.1822, -0.7126], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001,  0.0015], grad_fn=<StackBackward0>) tensor([-0.0001,  0.0015], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0080],\n","        [ 0.0138, -0.0076]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.020312393084168434\n","\n","Initial state: tensor([ 0.0799, -0.1829], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0797, -0.1837], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1829, -0.7105], grad_fn=<StackBackward0>) tensor([-0.1829, -0.7105], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-6.6235e-05,  8.7354e-05], grad_fn=<StackBackward0>) tensor([-6.6235e-05,  8.7354e-05], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0049],\n","        [0.0032, 0.0173]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02050017938017845\n","\n","Initial state: tensor([ 0.0797, -0.1837], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0795, -0.1844], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1837, -0.7084], grad_fn=<StackBackward0>) tensor([-0.1837, -0.7084], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0001, -0.0016], grad_fn=<StackBackward0>) tensor([ 0.0001, -0.0016], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0087],\n","        [-0.0141,  0.0136]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.020672166720032692\n","\n","Initial state: tensor([ 0.0795, -0.1844], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0793, -0.1851], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1844, -0.7064], grad_fn=<StackBackward0>) tensor([-0.1844, -0.7064], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([4.1217e-05, 8.8700e-06], grad_fn=<StackBackward0>) tensor([4.1217e-05, 8.8700e-06], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0031],\n","        [-0.0009, -0.0074]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02087453193962574\n","\n","Initial state: tensor([ 0.0793, -0.1851], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0792, -0.1858], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1851, -0.7043], grad_fn=<StackBackward0>) tensor([-0.1851, -0.7043], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003,  0.0016], grad_fn=<StackBackward0>) tensor([-0.0003,  0.0016], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0188],\n","        [ 0.0159, -0.0009]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.021045813336968422\n","\n","Initial state: tensor([ 0.0792, -0.1858], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0790, -0.1865], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1858, -0.7022], grad_fn=<StackBackward0>) tensor([-0.1858, -0.7022], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-8.9023e-05,  2.0427e-04], grad_fn=<StackBackward0>) tensor([-8.9023e-05,  2.0427e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0066],\n","        [0.0028, 0.0059]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.021265190094709396\n","\n","Initial state: tensor([ 0.0790, -0.1865], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0788, -0.1872], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1865, -0.7001], grad_fn=<StackBackward0>) tensor([-0.1865, -0.7001], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0005, -0.0021], grad_fn=<StackBackward0>) tensor([ 0.0005, -0.0021], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0338],\n","        [-0.0223, -0.0055]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.021455153822898865\n","\n","Initial state: tensor([ 0.0788, -0.1872], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0786, -0.1879], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1872, -0.6979], grad_fn=<StackBackward0>) tensor([-0.1872, -0.6979], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0005, -0.0019], grad_fn=<StackBackward0>) tensor([ 0.0005, -0.0019], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0345],\n","        [-0.0210, -0.0121]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.021612029522657394\n","\n","Initial state: tensor([ 0.0786, -0.1879], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0784, -0.1886], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1879, -0.6958], grad_fn=<StackBackward0>) tensor([-0.1879, -0.6958], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([5.1783e-06, 3.1395e-04], grad_fn=<StackBackward0>) tensor([5.1783e-06, 3.1395e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0004],\n","        [ 0.0018, -0.0101]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.021831251680850983\n","\n","Initial state: tensor([ 0.0784, -0.1886], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0782, -0.1893], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1886, -0.6937], grad_fn=<StackBackward0>) tensor([-0.1886, -0.6937], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0012], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0012], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0325],\n","        [0.0148, 0.0190]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.022025134414434433\n","\n","Initial state: tensor([ 0.0782, -0.1893], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0780, -0.1899], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1893, -0.6916], grad_fn=<StackBackward0>) tensor([-0.1893, -0.6916], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0004,  0.0006], grad_fn=<StackBackward0>) tensor([-0.0004,  0.0006], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0290],\n","        [0.0105, 0.0321]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02219570241868496\n","\n","Initial state: tensor([ 0.0780, -0.1899], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0778, -0.1906], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1899, -0.6894], grad_fn=<StackBackward0>) tensor([-0.1899, -0.6894], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 1.0038e-05, -6.9283e-04], grad_fn=<StackBackward0>) tensor([ 1.0038e-05, -6.9283e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0007],\n","        [-0.0037,  0.0240]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.022392941638827324\n","\n","Initial state: tensor([ 0.0778, -0.1906], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0776, -0.1913], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1906, -0.6873], grad_fn=<StackBackward0>) tensor([-0.1906, -0.6873], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0003, -0.0008], grad_fn=<StackBackward0>) tensor([ 0.0003, -0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0257],\n","        [-0.0098, -0.0108]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.022589730098843575\n","\n","Initial state: tensor([ 0.0776, -0.1913], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0775, -0.1920], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1913, -0.6851], grad_fn=<StackBackward0>) tensor([-0.1913, -0.6851], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0004], grad_fn=<StackBackward0>) tensor([0.0002, 0.0004], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0114],\n","        [-0.0008, -0.0320]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.022779326885938644\n","\n","Initial state: tensor([ 0.0775, -0.1920], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0773, -0.1927], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1920, -0.6830], grad_fn=<StackBackward0>) tensor([-0.1920, -0.6830], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0012], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0012], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0161],\n","        [ 0.0084, -0.0250]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.022962337359786034\n","\n","Initial state: tensor([ 0.0773, -0.1927], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0771, -0.1934], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1927, -0.6808], grad_fn=<StackBackward0>) tensor([-0.1927, -0.6808], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003,  0.0004], grad_fn=<StackBackward0>) tensor([-0.0003,  0.0004], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0197],\n","        [0.0046, 0.0037]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02316693216562271\n","\n","Initial state: tensor([ 0.0771, -0.1934], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0769, -0.1940], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1934, -0.6787], grad_fn=<StackBackward0>) tensor([-0.1934, -0.6787], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0002, -0.0012], grad_fn=<StackBackward0>) tensor([ 0.0002, -0.0012], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0161],\n","        [-0.0088,  0.0246]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0233660526573658\n","\n","Initial state: tensor([ 0.0769, -0.1940], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0767, -0.1947], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1940, -0.6765], grad_fn=<StackBackward0>) tensor([-0.1940, -0.6765], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0004, -0.0011], grad_fn=<StackBackward0>) tensor([ 0.0004, -0.0011], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0260],\n","        [-0.0082,  0.0211]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.023531731218099594\n","\n","Initial state: tensor([ 0.0767, -0.1947], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0765, -0.1954], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1947, -0.6743], grad_fn=<StackBackward0>) tensor([-0.1947, -0.6743], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([4.4423e-05, 2.7435e-04], grad_fn=<StackBackward0>) tensor([4.4423e-05, 2.7435e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0033],\n","        [ 0.0029,  0.0009]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.023778315633535385\n","\n","Initial state: tensor([ 0.0765, -0.1954], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0763, -0.1961], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1954, -0.6721], grad_fn=<StackBackward0>) tensor([-0.1954, -0.6721], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0006, -0.0013], grad_fn=<StackBackward0>) tensor([-0.0006, -0.0013], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0479],\n","        [-0.0157, -0.0231]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02398957684636116\n","\n","Initial state: tensor([ 0.0763, -0.1961], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0761, -0.1967], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1961, -0.6700], grad_fn=<StackBackward0>) tensor([-0.1961, -0.6700], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0008, -0.0015], grad_fn=<StackBackward0>) tensor([-0.0008, -0.0015], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0590],\n","        [-0.0199, -0.0331]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.024163462221622467\n","\n","Initial state: tensor([ 0.0761, -0.1967], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0759, -0.1974], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1967, -0.6678], grad_fn=<StackBackward0>) tensor([-0.1967, -0.6678], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0005, -0.0007], grad_fn=<StackBackward0>) tensor([-0.0005, -0.0007], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0355],\n","        [-0.0111, -0.0291]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.024333737790584564\n","\n","Initial state: tensor([ 0.0759, -0.1974], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0757, -0.1981], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1974, -0.6656], grad_fn=<StackBackward0>) tensor([-0.1974, -0.6656], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0008], grad_fn=<StackBackward0>) tensor([0.0002, 0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0155],\n","        [ 0.0072, -0.0083]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.024550534784793854\n","\n","Initial state: tensor([ 0.0757, -0.1981], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0755, -0.1987], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1981, -0.6634], grad_fn=<StackBackward0>) tensor([-0.1981, -0.6634], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0004, 0.0004], grad_fn=<StackBackward0>) tensor([0.0004, 0.0004], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0277],\n","        [ 0.0068,  0.0226]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.024747800081968307\n","\n","Initial state: tensor([ 0.0755, -0.1987], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0753, -0.1994], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1987, -0.6611], grad_fn=<StackBackward0>) tensor([-0.1987, -0.6611], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 0.0001, -0.0006], grad_fn=<StackBackward0>) tensor([ 0.0001, -0.0006], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0076],\n","        [-0.0019,  0.0331]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02494008094072342\n","\n","Initial state: tensor([ 0.0753, -0.1994], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0751, -0.2000], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.1994, -0.6589], grad_fn=<StackBackward0>) tensor([-0.1994, -0.6589], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0009], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0009], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0201],\n","        [-0.0073,  0.0152]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02513713389635086\n","\n","Initial state: tensor([ 0.0751, -0.2000], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0749, -0.2007], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2000, -0.6567], grad_fn=<StackBackward0>) tensor([-0.2000, -0.6567], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0002,  0.0003], grad_fn=<StackBackward0>) tensor([-0.0002,  0.0003], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0147],\n","        [ 0.0003, -0.0171]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02534661814570427\n","\n","Initial state: tensor([ 0.0749, -0.2007], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0747, -0.2014], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2007, -0.6545], grad_fn=<StackBackward0>) tensor([-0.2007, -0.6545], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0002, 0.0010], grad_fn=<StackBackward0>) tensor([0.0002, 0.0010], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0157],\n","        [ 0.0065, -0.0250]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0255426075309515\n","\n","Initial state: tensor([ 0.0747, -0.2014], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0745, -0.2020], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2014, -0.6522], grad_fn=<StackBackward0>) tensor([-0.2014, -0.6522], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([0.0003, 0.0004], grad_fn=<StackBackward0>) tensor([0.0003, 0.0004], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0218],\n","        [ 0.0024, -0.0091]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.025741402059793472\n","\n","Initial state: tensor([ 0.0745, -0.2020], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0743, -0.2027], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2020, -0.6500], grad_fn=<StackBackward0>) tensor([-0.2020, -0.6500], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0001, -0.0008], grad_fn=<StackBackward0>) tensor([-0.0001, -0.0008], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0100],\n","        [-0.0059,  0.0160]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.025949643924832344\n","\n","Initial state: tensor([ 0.0743, -0.2027], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0741, -0.2033], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2027, -0.6478], grad_fn=<StackBackward0>) tensor([-0.2027, -0.6478], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-0.0003, -0.0002], grad_fn=<StackBackward0>) tensor([-0.0003, -0.0002], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000e+00, 1.8948e-02],\n","        [6.6757e-05, 1.5858e-02]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02613561414182186\n","\n","Initial state: tensor([ 0.0741, -0.2033], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0739, -0.2039], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2033, -0.6455], grad_fn=<StackBackward0>) tensor([-0.2033, -0.6455], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([3.9511e-05, 5.5935e-04], grad_fn=<StackBackward0>) tensor([3.9511e-05, 5.5935e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0029],\n","        [ 0.0053, -0.0022]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.026362719014286995\n","\n","Initial state: tensor([ 0.0739, -0.2039], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0737, -0.2046], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2039, -0.6433], grad_fn=<StackBackward0>) tensor([-0.2039, -0.6433], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 5.4567e-05, -2.5752e-03], grad_fn=<StackBackward0>) tensor([ 5.4567e-05, -2.5752e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0040],\n","        [-0.0270, -0.0089]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.026554660871624947\n","\n","Initial state: tensor([ 0.0737, -0.2046], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0735, -0.2052], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2046, -0.6410], grad_fn=<StackBackward0>) tensor([-0.2046, -0.6410], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-1.4931e-05, -1.2969e-03], grad_fn=<StackBackward0>) tensor([-1.4931e-05, -1.2969e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0011],\n","        [-0.0138, -0.0062]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02678471803665161\n","\n","Initial state: tensor([ 0.0735, -0.2052], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0733, -0.2059], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2052, -0.6387], grad_fn=<StackBackward0>) tensor([-0.2052, -0.6387], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-4.5688e-05,  3.7200e-03], grad_fn=<StackBackward0>) tensor([-4.5688e-05,  3.7200e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0034],\n","        [0.0383, 0.0077]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.026996608823537827\n","\n","Initial state: tensor([ 0.0733, -0.2059], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0731, -0.2065], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2059, -0.6365], grad_fn=<StackBackward0>) tensor([-0.2059, -0.6365], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-3.4130e-05,  3.9803e-03], grad_fn=<StackBackward0>) tensor([-3.4130e-05,  3.9803e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0025],\n","        [0.0418, 0.0146]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02717180736362934\n","\n","Initial state: tensor([ 0.0731, -0.2065], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0729, -0.2071], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2065, -0.6342], grad_fn=<StackBackward0>) tensor([-0.2065, -0.6342], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-2.6620e-07,  2.2948e-04], grad_fn=<StackBackward0>) tensor([-2.6620e-07,  2.2948e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000e+00, 1.9729e-05],\n","        [3.8605e-03, 1.1580e-02]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.027415316551923752\n","\n","Initial state: tensor([ 0.0729, -0.2071], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0727, -0.2078], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2071, -0.6319], grad_fn=<StackBackward0>) tensor([-0.2071, -0.6319], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 2.3775e-05, -4.1711e-03], grad_fn=<StackBackward0>) tensor([ 2.3775e-05, -4.1711e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0018],\n","        [-0.0442, -0.0179]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.027634186670184135\n","\n","Initial state: tensor([ 0.0727, -0.2078], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0724, -0.2084], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2078, -0.6296], grad_fn=<StackBackward0>) tensor([-0.2078, -0.6296], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 1.8059e-05, -4.2317e-03], grad_fn=<StackBackward0>) tensor([ 1.8059e-05, -4.2317e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0013],\n","        [-0.0469, -0.0339]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02782045118510723\n","\n","Initial state: tensor([ 0.0724, -0.2084], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0722, -0.2090], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2084, -0.6273], grad_fn=<StackBackward0>) tensor([-0.2084, -0.6273], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-7.0349e-06, -1.0135e-03], grad_fn=<StackBackward0>) tensor([-7.0349e-06, -1.0135e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0005],\n","        [-0.0144, -0.0318]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02802835963666439\n","\n","Initial state: tensor([ 0.0722, -0.2090], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0720, -0.2096], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2090, -0.6250], grad_fn=<StackBackward0>) tensor([-0.2090, -0.6250], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-2.8945e-05,  3.3206e-03], grad_fn=<StackBackward0>) tensor([-2.8945e-05,  3.3206e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000,  0.0021],\n","        [ 0.0327, -0.0043]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.02824622392654419\n","\n","Initial state: tensor([ 0.0720, -0.2096], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0718, -0.2103], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2096, -0.6227], grad_fn=<StackBackward0>) tensor([-0.2096, -0.6227], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([-2.0683e-05,  2.9009e-03], grad_fn=<StackBackward0>) tensor([-2.0683e-05,  2.9009e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[0.0000, 0.0015],\n","        [0.0323, 0.0242]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.0284498892724514\n","\n","Initial state: tensor([ 0.0718, -0.2103], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0716, -0.2109], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2103, -0.6204], grad_fn=<StackBackward0>) tensor([-0.2103, -0.6204], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 5.7918e-08, -6.5834e-04], grad_fn=<StackBackward0>) tensor([ 5.7918e-08, -6.5834e-04], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000e+00, -4.2915e-06],\n","        [-2.1477e-03,  3.2881e-02]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.028661590069532394\n","\n","Initial state: tensor([ 0.0716, -0.2109], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0714, -0.2115], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2109, -0.6181], grad_fn=<StackBackward0>) tensor([-0.2109, -0.6181], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 1.1243e-05, -3.2094e-03], grad_fn=<StackBackward0>) tensor([ 1.1243e-05, -3.2094e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000, -0.0008],\n","        [-0.0304,  0.0125]], grad_fn=<SubBackward0>)\n","\n","Loss: 0.028864430263638496\n","\n","Initial state: tensor([ 0.0714, -0.2115], requires_grad=True)\n","Initial state NN: tensor([ 0.0999, -0.0135], requires_grad=True)\n","New state: tensor([ 0.0712, -0.2121], grad_fn=<AddBackward0>)\n","New state NN: tensor([ 0.0999, -0.0135], grad_fn=<AddBackward0>)\n","\n","Dynamics at initial state: tensor([-0.2115, -0.6158], grad_fn=<StackBackward0>) tensor([-0.2115, -0.6158], grad_fn=<MvBackward0>)\n","Dynamics at initial state NN: tensor([ 3.2186e-09, -1.4193e-03], grad_fn=<StackBackward0>) tensor([ 3.2186e-09, -1.4193e-03], grad_fn=<MvBackward0>)\n","\n","Jacobian matrix initial: tensor([[-0.0000,  1.0000],\n","        [-9.8100, -0.4000]])\n","Jacobian matrix initial NN: tensor([[ 0.0000e+00, -2.3842e-07],\n","        [-1.6454e-02, -1.6645e-02]], grad_fn=<SubBackward0>)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-52-e2a8f54867ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    134\u001b[0m   \u001b[0;31m# loss += grad_penalty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_weights.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["w"],"metadata":{"id":"73kO8z3Lz3Am"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f6dd9669a4ea48968a9fc5bba3f867e6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52315fd26bdd487b9c32862c6aedcbfe","IPY_MODEL_62a6f9366a374c5c8483933f40bc7bf7","IPY_MODEL_a1227982dd9a46d1bde7e68040cb90a1"],"layout":"IPY_MODEL_b34baf01a88141e5923630db1b7e0f6f"}},"52315fd26bdd487b9c32862c6aedcbfe":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6328b85ce9f244f1afc26545b005e6f5","placeholder":"​","style":"IPY_MODEL_4cb6ae5c2ce94350af27504f0708b738","value":" 12%"}},"62a6f9366a374c5c8483933f40bc7bf7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbf12916900d4e928b01e3f832ac9de2","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19a52192be46414d8bfd45e32b71d989","value":12}},"a1227982dd9a46d1bde7e68040cb90a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3484a29efc4e4100af871edfb725d870","placeholder":"​","style":"IPY_MODEL_c626ef44d5a14987b0a05cb39428fd98","value":" 12/100 [09:18&lt;1:10:01, 47.74s/it]"}},"b34baf01a88141e5923630db1b7e0f6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6328b85ce9f244f1afc26545b005e6f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4cb6ae5c2ce94350af27504f0708b738":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bbf12916900d4e928b01e3f832ac9de2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19a52192be46414d8bfd45e32b71d989":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3484a29efc4e4100af871edfb725d870":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c626ef44d5a14987b0a05cb39428fd98":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4a45ceb540b4fad9be7d05aab115fc6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8f9f886562b2463896306873434b8bc2","IPY_MODEL_fc05af248f3a410190315da7b3c89c3b","IPY_MODEL_046dbe14e7634640a5856afc579f1057"],"layout":"IPY_MODEL_824265b31b84403fa1d9e5efc2845b11"}},"8f9f886562b2463896306873434b8bc2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_154c5ac95d224335a80b4b920146a9dd","placeholder":"​","style":"IPY_MODEL_868a9d3ad614451ba8c297a1e88f645c","value":"100%"}},"fc05af248f3a410190315da7b3c89c3b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6ff923f134f148ac8b487e0f6ba23cb1","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_316429c2ad5e4913b8d928e0d6cb1b72","value":1599}},"046dbe14e7634640a5856afc579f1057":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ccb3da09bee4b4195cb04fbfedb5def","placeholder":"​","style":"IPY_MODEL_89a7cf2e45b84fad996248c30bc9c6eb","value":" 1599/1599 [00:39&lt;00:00, 42.06it/s]"}},"824265b31b84403fa1d9e5efc2845b11":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"154c5ac95d224335a80b4b920146a9dd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"868a9d3ad614451ba8c297a1e88f645c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ff923f134f148ac8b487e0f6ba23cb1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"316429c2ad5e4913b8d928e0d6cb1b72":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ccb3da09bee4b4195cb04fbfedb5def":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89a7cf2e45b84fad996248c30bc9c6eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8d3df87dd9b4b8b9caaba9ffb811d37":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_87b873333b7e49e3a740c4a6ad20bbd1","IPY_MODEL_c09d5000e80e4923af0e30c2fc9742d5","IPY_MODEL_615917daa2c5433ca67b6fadb227d5ce"],"layout":"IPY_MODEL_4bf5d4dbf3cf4baeb0fb26f83899bac9"}},"87b873333b7e49e3a740c4a6ad20bbd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae7fc880db6d4afe91d6073b1873f978","placeholder":"​","style":"IPY_MODEL_1f74114e9a68464eabef8fbbe213d69d","value":"100%"}},"c09d5000e80e4923af0e30c2fc9742d5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eff314a726f94aa5a3c1ade5e1b0cdc7","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5647d0bb250d482f90020c197e64a668","value":1599}},"615917daa2c5433ca67b6fadb227d5ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d90c76faecff43f98718dc3d2ccc5e8e","placeholder":"​","style":"IPY_MODEL_9152de694c974a3ca9a4943ad7d68cc1","value":" 1599/1599 [00:38&lt;00:00, 36.06it/s]"}},"4bf5d4dbf3cf4baeb0fb26f83899bac9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae7fc880db6d4afe91d6073b1873f978":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f74114e9a68464eabef8fbbe213d69d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eff314a726f94aa5a3c1ade5e1b0cdc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5647d0bb250d482f90020c197e64a668":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d90c76faecff43f98718dc3d2ccc5e8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9152de694c974a3ca9a4943ad7d68cc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac2de50d560e45dba02a9c5234214881":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2aee738559844fe3a16e639a46b409d6","IPY_MODEL_c4ba64860f264c7db085d74c18279a18","IPY_MODEL_e9eedacf1fd24a77986f5d7c746107a4"],"layout":"IPY_MODEL_ddc72aa36d4249b8b1bc3b7786661696"}},"2aee738559844fe3a16e639a46b409d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86b1e11e01e648a595489642b7265cbf","placeholder":"​","style":"IPY_MODEL_49e57733793946b4bcbe1a0c63a10744","value":"100%"}},"c4ba64860f264c7db085d74c18279a18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_742d7d14029e4fc69e2a08566af9cb34","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80364bf0e66446d6874dc4e4901ae5f8","value":1599}},"e9eedacf1fd24a77986f5d7c746107a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3992b684111847689c0ef7836e80ed2a","placeholder":"​","style":"IPY_MODEL_4773e75053dd4a5c87681aebbe30901d","value":" 1599/1599 [00:48&lt;00:00, 45.29it/s]"}},"ddc72aa36d4249b8b1bc3b7786661696":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86b1e11e01e648a595489642b7265cbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49e57733793946b4bcbe1a0c63a10744":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"742d7d14029e4fc69e2a08566af9cb34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80364bf0e66446d6874dc4e4901ae5f8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3992b684111847689c0ef7836e80ed2a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4773e75053dd4a5c87681aebbe30901d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"995fd94e3ee54f93850f1f95670c9384":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bb42917a8ee04d6f91cce5ba5db3b1d1","IPY_MODEL_e6f4353a1551435b90e38440acef172b","IPY_MODEL_a85af070ce39407c8cdf56cdf2e045de"],"layout":"IPY_MODEL_28898b0649a149afb2d6ef5ce7b247e8"}},"bb42917a8ee04d6f91cce5ba5db3b1d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_06939cfaf1fc4886b8f2e7ff1a2b228f","placeholder":"​","style":"IPY_MODEL_24086fcbfb984504a01fed20925dfa34","value":"100%"}},"e6f4353a1551435b90e38440acef172b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_acfa892c4aba478388fe2f1cbead4364","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca9edadaecdc487d9dc6bf5f4745e000","value":1599}},"a85af070ce39407c8cdf56cdf2e045de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca06e55dca03490fb85b6dbababa6248","placeholder":"​","style":"IPY_MODEL_dc7e14ac33494051a842f2400cac2b84","value":" 1599/1599 [00:40&lt;00:00, 29.64it/s]"}},"28898b0649a149afb2d6ef5ce7b247e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06939cfaf1fc4886b8f2e7ff1a2b228f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24086fcbfb984504a01fed20925dfa34":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acfa892c4aba478388fe2f1cbead4364":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca9edadaecdc487d9dc6bf5f4745e000":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca06e55dca03490fb85b6dbababa6248":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc7e14ac33494051a842f2400cac2b84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d992b7eb37194f5ab98267064a14a8eb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ad3fc896be4e44558cb6afd8da893c93","IPY_MODEL_4e5d073f34084184b3f513d8ffad5860","IPY_MODEL_d907859afc504eeca24dae37b4473efc"],"layout":"IPY_MODEL_bd0725bcb75540e9bdbadbccabf66ad3"}},"ad3fc896be4e44558cb6afd8da893c93":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebd27a23888e49cda189653c6e23db4f","placeholder":"​","style":"IPY_MODEL_17199da3363443f2b5c954bcb23cc851","value":"100%"}},"4e5d073f34084184b3f513d8ffad5860":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_956c4125e78b4bf6a23d1f2a94745db7","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac340ec5ce0643e5b85825e1932ddbbc","value":1599}},"d907859afc504eeca24dae37b4473efc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13eb9bc705ab4b1db11ef5563d9fbc21","placeholder":"​","style":"IPY_MODEL_5469735a236f47b08dbdc200bff11b1d","value":" 1599/1599 [00:44&lt;00:00, 29.75it/s]"}},"bd0725bcb75540e9bdbadbccabf66ad3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebd27a23888e49cda189653c6e23db4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17199da3363443f2b5c954bcb23cc851":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"956c4125e78b4bf6a23d1f2a94745db7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac340ec5ce0643e5b85825e1932ddbbc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"13eb9bc705ab4b1db11ef5563d9fbc21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5469735a236f47b08dbdc200bff11b1d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5db5329cfdf6470aa5001a6915d5e41e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5dbe6f822e5c48eea419ad835dcb9de2","IPY_MODEL_71fbb66750494cbeb54e6231655eba18","IPY_MODEL_05905b08de0344b19eff931f94d4bbd8"],"layout":"IPY_MODEL_a69508dd4516429aa34eab73c68c4faa"}},"5dbe6f822e5c48eea419ad835dcb9de2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c89044cd917546ed8b136530511cf613","placeholder":"​","style":"IPY_MODEL_cf4fa89f68df4305a0e72901d676bb01","value":"100%"}},"71fbb66750494cbeb54e6231655eba18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff504286e0ad4da4af70c82e6edd8d55","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5a3c413bb61848039e5950c83c64ed9a","value":1599}},"05905b08de0344b19eff931f94d4bbd8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f868ca8e92e48178a0f4ee203bc9cf3","placeholder":"​","style":"IPY_MODEL_11f50c78362b4885a53d7889b6875175","value":" 1599/1599 [00:42&lt;00:00, 30.81it/s]"}},"a69508dd4516429aa34eab73c68c4faa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c89044cd917546ed8b136530511cf613":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf4fa89f68df4305a0e72901d676bb01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ff504286e0ad4da4af70c82e6edd8d55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a3c413bb61848039e5950c83c64ed9a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4f868ca8e92e48178a0f4ee203bc9cf3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"11f50c78362b4885a53d7889b6875175":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0529760966984790800db92cdb8b4e07":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8dc77f9fa39470ab19b9bdcc2289355","IPY_MODEL_4803d4094f664123b0afa6deee10cd7e","IPY_MODEL_bc8fbfb3adaa4755a5eeebed932b2aab"],"layout":"IPY_MODEL_1bc90544697145af9634d0f5446b38fe"}},"c8dc77f9fa39470ab19b9bdcc2289355":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c47d4c7f76c40de87a65be237209884","placeholder":"​","style":"IPY_MODEL_f27295f8975049b98d030e963d0d6aaa","value":"100%"}},"4803d4094f664123b0afa6deee10cd7e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed046ee7ce3746e8ab9fa17837fd505c","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0656baf49a4547c89900e6cd72ae73ef","value":1599}},"bc8fbfb3adaa4755a5eeebed932b2aab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_38471fd75a42460cb29ed6dd07b6fd20","placeholder":"​","style":"IPY_MODEL_4392703b66d64b0d89d57f0bb9e931af","value":" 1599/1599 [00:41&lt;00:00, 34.10it/s]"}},"1bc90544697145af9634d0f5446b38fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c47d4c7f76c40de87a65be237209884":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f27295f8975049b98d030e963d0d6aaa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed046ee7ce3746e8ab9fa17837fd505c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0656baf49a4547c89900e6cd72ae73ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"38471fd75a42460cb29ed6dd07b6fd20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4392703b66d64b0d89d57f0bb9e931af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f56179127f9c4baa8bcb247cbf0f92c6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a16452ac7a5345148a218eef756c8313","IPY_MODEL_4d8aa4f33e134981a65ee0f00eee67a5","IPY_MODEL_16a9fe09de4645d1b40feacc901faea3"],"layout":"IPY_MODEL_82ee794409b5494e84ad33f1db42f015"}},"a16452ac7a5345148a218eef756c8313":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44a8ed5d7b84485dafbcd6c68ae9ffe6","placeholder":"​","style":"IPY_MODEL_980e60867fe64078ae57ff0692eb35eb","value":"100%"}},"4d8aa4f33e134981a65ee0f00eee67a5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3650fed6ffa24a64ae7d9eadfde22686","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9a870d20fe284afab2ee2d385db8df27","value":1599}},"16a9fe09de4645d1b40feacc901faea3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65d1e0529f584e48a76c57fdd7a089e1","placeholder":"​","style":"IPY_MODEL_3044badb741e4d9dafe15ddc0130d820","value":" 1599/1599 [00:43&lt;00:00, 38.92it/s]"}},"82ee794409b5494e84ad33f1db42f015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44a8ed5d7b84485dafbcd6c68ae9ffe6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"980e60867fe64078ae57ff0692eb35eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3650fed6ffa24a64ae7d9eadfde22686":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a870d20fe284afab2ee2d385db8df27":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65d1e0529f584e48a76c57fdd7a089e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3044badb741e4d9dafe15ddc0130d820":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7add205d98ef49509dca52b02f536462":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e45d42ff8b0a4bdd8dbeaa8bb06d3724","IPY_MODEL_fa61776497df43cd945b1c91d845576b","IPY_MODEL_c3c0227675e1465c97c082754edb62dc"],"layout":"IPY_MODEL_de478ba1854c4d64bd2977f58d6d2dbf"}},"e45d42ff8b0a4bdd8dbeaa8bb06d3724":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a213e287bbe467fa2a3842478b8765c","placeholder":"​","style":"IPY_MODEL_6de62069d06f487cb3a5306b98f38bf0","value":"100%"}},"fa61776497df43cd945b1c91d845576b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f3b73a8c41684c0e82dfde81154dcbd2","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_25a7776fe5864122a706c4b934ef7cd3","value":1599}},"c3c0227675e1465c97c082754edb62dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a44a012f368742a8b4005ede7fbc5c41","placeholder":"​","style":"IPY_MODEL_e22511f44a49460880f670f950bcd45e","value":" 1599/1599 [00:46&lt;00:00, 28.99it/s]"}},"de478ba1854c4d64bd2977f58d6d2dbf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a213e287bbe467fa2a3842478b8765c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6de62069d06f487cb3a5306b98f38bf0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3b73a8c41684c0e82dfde81154dcbd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"25a7776fe5864122a706c4b934ef7cd3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a44a012f368742a8b4005ede7fbc5c41":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e22511f44a49460880f670f950bcd45e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51c3d58078dc4118a72633615d5ba72e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7201fbce23d949f197c19a81ca1fce4b","IPY_MODEL_f11725a748684a9d9ff547e2ef2fb761","IPY_MODEL_c4f170c0d3014fca95dcc9f9662ad5d4"],"layout":"IPY_MODEL_333def2aed184b299c436088281ca8cc"}},"7201fbce23d949f197c19a81ca1fce4b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ae875200ef7b4c7cbc0bbae1d270d110","placeholder":"​","style":"IPY_MODEL_f55b67583360460e9c4c065e0f97dc36","value":"100%"}},"f11725a748684a9d9ff547e2ef2fb761":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f74be5a60784a778f50fd4d27cc1353","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9d2cb7e3e764c4c9774786d561f5d74","value":1599}},"c4f170c0d3014fca95dcc9f9662ad5d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bc2ae44e59444febb7f1f9727e5493c2","placeholder":"​","style":"IPY_MODEL_002af59acb8f40149057dee92cfbac78","value":" 1599/1599 [00:43&lt;00:00, 39.82it/s]"}},"333def2aed184b299c436088281ca8cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae875200ef7b4c7cbc0bbae1d270d110":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f55b67583360460e9c4c065e0f97dc36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6f74be5a60784a778f50fd4d27cc1353":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9d2cb7e3e764c4c9774786d561f5d74":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bc2ae44e59444febb7f1f9727e5493c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"002af59acb8f40149057dee92cfbac78":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c93c30d908540428b6380dfe548132f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b1a229b1f0f47ba911d875077a29100","IPY_MODEL_bbf8da4357244ceba29a02d92bff99f3","IPY_MODEL_e7f249c8a0e14a29a1175891fd10a63c"],"layout":"IPY_MODEL_e0804d71748448069fd3f090f698d636"}},"3b1a229b1f0f47ba911d875077a29100":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5ea40597930491aa1dabb12b790a6c7","placeholder":"​","style":"IPY_MODEL_f137c7d81aa94680a36345e70bc4b641","value":"100%"}},"bbf8da4357244ceba29a02d92bff99f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8845edef4b61458bbadab90c8b98552c","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16860f5a47df4daaae78e6fbacb91909","value":1599}},"e7f249c8a0e14a29a1175891fd10a63c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_413d2244d6de4685b1aa37f68aefb7ff","placeholder":"​","style":"IPY_MODEL_70428e4dabb34c8d965ceeced5f9dcd9","value":" 1599/1599 [00:43&lt;00:00, 40.60it/s]"}},"e0804d71748448069fd3f090f698d636":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5ea40597930491aa1dabb12b790a6c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f137c7d81aa94680a36345e70bc4b641":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8845edef4b61458bbadab90c8b98552c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16860f5a47df4daaae78e6fbacb91909":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"413d2244d6de4685b1aa37f68aefb7ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70428e4dabb34c8d965ceeced5f9dcd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f3bea8fa58c34accb25cee476b53e7f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_df44a2e7fd674e0486daba741a728fce","IPY_MODEL_52bf2254a4eb439cb0d7324b11665e43","IPY_MODEL_f4cda6662d53443cb129c78e3ca6ce95"],"layout":"IPY_MODEL_8717e6da71004e9a9e32ce3cd556a837"}},"df44a2e7fd674e0486daba741a728fce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_742cf7dce53349288ec2da3503783ffd","placeholder":"​","style":"IPY_MODEL_544edd53bcf0487b825491044e9a7302","value":"100%"}},"52bf2254a4eb439cb0d7324b11665e43":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_71bc5522b6434826a3d7aaf132631243","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e04a2a0f170b49019d9f05d89eb35520","value":1599}},"f4cda6662d53443cb129c78e3ca6ce95":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cea1c54b831b4f0a8adb6af25371f715","placeholder":"​","style":"IPY_MODEL_9c0ea85133cc49e99e7c56cae20e3a47","value":" 1599/1599 [00:44&lt;00:00, 42.96it/s]"}},"8717e6da71004e9a9e32ce3cd556a837":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"742cf7dce53349288ec2da3503783ffd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"544edd53bcf0487b825491044e9a7302":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71bc5522b6434826a3d7aaf132631243":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e04a2a0f170b49019d9f05d89eb35520":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cea1c54b831b4f0a8adb6af25371f715":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c0ea85133cc49e99e7c56cae20e3a47":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"610319b49fe24bc1ab5e6b9e0beee98a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_625daf1877f340a5be334c30e08631cd","IPY_MODEL_3fe513d2172746bd8d87eb12a8f9200e","IPY_MODEL_ba514096a0634363b5d9f80439df79b7"],"layout":"IPY_MODEL_6f56ec957779454f81b74772ae191cd5"}},"625daf1877f340a5be334c30e08631cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e444ff22a9334dca96e7d8158ff413f9","placeholder":"​","style":"IPY_MODEL_13a45ec56d4c4179941b6df3d753c1bf","value":" 85%"}},"3fe513d2172746bd8d87eb12a8f9200e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b729d71b72ed497b92e209ce9dbc1950","max":1599,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f634ba2b00f340c5a169793f2f003cd7","value":1359}},"ba514096a0634363b5d9f80439df79b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df8dce8136d74360863cdf4668773da3","placeholder":"​","style":"IPY_MODEL_50f7c6412a69403981c079aeb7b1c8b9","value":" 1354/1599 [00:36&lt;00:05, 40.99it/s]"}},"6f56ec957779454f81b74772ae191cd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e444ff22a9334dca96e7d8158ff413f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13a45ec56d4c4179941b6df3d753c1bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b729d71b72ed497b92e209ce9dbc1950":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f634ba2b00f340c5a169793f2f003cd7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df8dce8136d74360863cdf4668773da3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50f7c6412a69403981c079aeb7b1c8b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}